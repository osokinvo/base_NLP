{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Стемминг<h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Стемминг – это своего рода нормализация слов. Нормализация – это метод, при котором набор слов в предложении преобразуется в последовательность, чтобы сократить время поиска. Слова, которые имеют то же значение, но имеют некоторые различия в зависимости от контекста или предложения, нормализуются. Другими словами, есть одно корневое слово, но есть много вариантов одних и тех же слов. Например, корневое слово «есть» и его вариации «есть, есть, есть и так далее». Точно так же, с помощью Stemming, мы можем найти корневое слово любых вариаций.<h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Импортируем библиотеки<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/val/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/val/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import LancasterStemmer\n",
    "from model_selction import model_selection_word_count, model_selection_word_exist, model_selection_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mmisspel.csv\u001b[0m*     \u001b[01;32mprocessedNegative.csv\u001b[0m*  \u001b[01;32mprocessedPositive.csv\u001b[0m*\n",
      "\u001b[01;32mp00_tweets.zip\u001b[0m*  \u001b[01;32mprocessedNeutral.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>В качестве примера рассмотрим содержимое файла 'processedNegative.csv' после применения метода<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'unhappy',\n",
       " 'dog',\n",
       " 'lik',\n",
       " 'though',\n",
       " 'talk',\n",
       " 'driv',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'goingh',\n",
       " 'said',\n",
       " \"'d\",\n",
       " 'lov',\n",
       " 'go',\n",
       " 'new',\n",
       " 'york',\n",
       " 'sint',\n",
       " 'trump',\n",
       " \"'s\",\n",
       " 'prob',\n",
       " 'doe',\n",
       " 'anybody',\n",
       " 'know',\n",
       " 'rand',\n",
       " \"'s\",\n",
       " 'lik',\n",
       " 'fal',\n",
       " 'doll',\n",
       " '?',\n",
       " 'i',\n",
       " 'got',\n",
       " 'money',\n",
       " 'i',\n",
       " 'nee',\n",
       " 'chang',\n",
       " 'r',\n",
       " 'keep',\n",
       " 'get',\n",
       " 'stronger',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'going',\n",
       " 'gig',\n",
       " 'liverpool',\n",
       " 'unhappy',\n",
       " 'ther',\n",
       " 'isnt',\n",
       " 'new',\n",
       " 'riverd',\n",
       " 'tonight',\n",
       " '?',\n",
       " 'unhappy',\n",
       " \"'s\",\n",
       " 'a',\n",
       " '*',\n",
       " 'dy',\n",
       " 'guy',\n",
       " 'pop',\n",
       " 'as',\n",
       " 'transl',\n",
       " \"'ll\",\n",
       " 'prob',\n",
       " 'go',\n",
       " 'around',\n",
       " 'au',\n",
       " 'unhappy',\n",
       " 'who',\n",
       " \"'s\",\n",
       " 'chair',\n",
       " \"'re\",\n",
       " 'sit',\n",
       " '?',\n",
       " 'is',\n",
       " 'i',\n",
       " 'find',\n",
       " '.',\n",
       " 'everyon',\n",
       " 'know',\n",
       " '.',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'sham',\n",
       " 'pu',\n",
       " \"n't\",\n",
       " 'lik',\n",
       " 'jittery',\n",
       " 'caffein',\n",
       " 'mak',\n",
       " 'sad',\n",
       " 'my',\n",
       " 'are',\n",
       " \"'s\",\n",
       " 'list',\n",
       " 'unhappy',\n",
       " 'think',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'go',\n",
       " 'libdem',\n",
       " 'anyway',\n",
       " 'i',\n",
       " 'want',\n",
       " 'fun',\n",
       " 'plan',\n",
       " 'weekend',\n",
       " 'unhappy',\n",
       " 'when',\n",
       " 'not',\n",
       " '.',\n",
       " 'unhappy',\n",
       " '?',\n",
       " 'ahhhhh',\n",
       " '!',\n",
       " 'you',\n",
       " 'recogn',\n",
       " 'log',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'cinemax',\n",
       " 'show',\n",
       " 'bad',\n",
       " 'track',\n",
       " 'record',\n",
       " 'get',\n",
       " 'cancel',\n",
       " 'unhappy',\n",
       " 'err',\n",
       " 'dud',\n",
       " '....',\n",
       " 'they',\n",
       " \"'re\",\n",
       " 'gon',\n",
       " 'unhappy',\n",
       " 'ask',\n",
       " 'leagu',\n",
       " 'memeb',\n",
       " 'check',\n",
       " 'guy',\n",
       " 'go',\n",
       " 'not',\n",
       " 'sad',\n",
       " 'why',\n",
       " 'would',\n",
       " 'harvey',\n",
       " 'going',\n",
       " 'prison',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'miss',\n",
       " 'cry',\n",
       " 'seasid',\n",
       " 'are',\n",
       " '.',\n",
       " 'becoz',\n",
       " 'depend',\n",
       " 'promot',\n",
       " 'wast',\n",
       " 'hardwork',\n",
       " 'team',\n",
       " 'i',\n",
       " 'thought',\n",
       " \"'ll\",\n",
       " 'sav',\n",
       " 'cry',\n",
       " 'maj',\n",
       " 'waffl',\n",
       " 'crav',\n",
       " 'right',\n",
       " 'sad',\n",
       " 'cant',\n",
       " 'speak',\n",
       " 'japanes',\n",
       " ':',\n",
       " ':',\n",
       " '(',\n",
       " 'peopl',\n",
       " 'stuff',\n",
       " 'lik',\n",
       " 'unhappy',\n",
       " 'pleas',\n",
       " 'stop',\n",
       " 'confin',\n",
       " 'anim',\n",
       " 'zoo',\n",
       " 'unhappy',\n",
       " 'feel',\n",
       " 'lik',\n",
       " 'shoyould',\n",
       " 'tel',\n",
       " 'get',\n",
       " 'fuck',\n",
       " 'soc',\n",
       " 'med',\n",
       " 'byout',\n",
       " 'also',\n",
       " 'feel',\n",
       " 'real',\n",
       " 'mean',\n",
       " 'unhappy',\n",
       " 'sil',\n",
       " 'lov',\n",
       " 'yoyou',\n",
       " 'hop',\n",
       " 'yoyo',\n",
       " 'okay',\n",
       " 'miss',\n",
       " 'huhu',\n",
       " 'busy',\n",
       " 'unhappy',\n",
       " 'extend',\n",
       " 'famy',\n",
       " '.',\n",
       " '12',\n",
       " 'ppl.ahh',\n",
       " 'want',\n",
       " 'show',\n",
       " 'oh',\n",
       " 'my',\n",
       " 'girl',\n",
       " 'dorky',\n",
       " 'play',\n",
       " 'gam',\n",
       " 'got',\n",
       " 'delet',\n",
       " 'unhappy',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'unhappy',\n",
       " 'jamy',\n",
       " 'pleas',\n",
       " 'reset',\n",
       " 'cga',\n",
       " 'grandfin',\n",
       " 'serv',\n",
       " '...',\n",
       " 'admin',\n",
       " 'respond',\n",
       " 'unhappy',\n",
       " 'noooooooo',\n",
       " 'you',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'miss',\n",
       " 'the',\n",
       " 'buffet',\n",
       " 'unhappy',\n",
       " 'tak',\n",
       " 'car',\n",
       " 'ain',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'i',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'vot',\n",
       " 'unhappy',\n",
       " 'inst',\n",
       " 'mess',\n",
       " 'jeal',\n",
       " 'okay',\n",
       " 'unhappy',\n",
       " 'nev',\n",
       " 'mind',\n",
       " 'hah',\n",
       " 'bruno',\n",
       " 'wait',\n",
       " 'fin',\n",
       " 'first',\n",
       " \"'m\",\n",
       " 'enl',\n",
       " 'pleas',\n",
       " 'turn',\n",
       " 'lik',\n",
       " 'unhappy',\n",
       " 'unhappy',\n",
       " 'com',\n",
       " 'peopl',\n",
       " 'lik',\n",
       " 'childr',\n",
       " \"'s\",\n",
       " 'stat',\n",
       " 'interv',\n",
       " 'ouchhhhh',\n",
       " 'unhappy',\n",
       " 'help',\n",
       " '...',\n",
       " 'i',\n",
       " 'want',\n",
       " 'stop',\n",
       " 'tweet',\n",
       " '.',\n",
       " 'al',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'endless',\n",
       " 'suff',\n",
       " 'pain',\n",
       " '.',\n",
       " 'i',\n",
       " 'tri',\n",
       " 'deact',\n",
       " 'many',\n",
       " 'tim',\n",
       " '...',\n",
       " 'sav',\n",
       " '...',\n",
       " 'unhappy',\n",
       " 'for',\n",
       " 'ask',\n",
       " 'apply',\n",
       " 'kan',\n",
       " 'kanj',\n",
       " 'funtim',\n",
       " '!',\n",
       " 'sad',\n",
       " 'look',\n",
       " 'lik',\n",
       " 'io',\n",
       " '11',\n",
       " 'due',\n",
       " 'kil',\n",
       " 'unhappy',\n",
       " 'yeah',\n",
       " 'upd',\n",
       " '16.04',\n",
       " 'froz',\n",
       " 'tim',\n",
       " '.',\n",
       " 'then',\n",
       " 'went',\n",
       " '16.10',\n",
       " 'froz',\n",
       " 'mid',\n",
       " 'instal',\n",
       " '.',\n",
       " 'wait',\n",
       " '3hrs',\n",
       " \"'d\",\n",
       " 'pul',\n",
       " 'plug',\n",
       " 'cry',\n",
       " 'shaanda',\n",
       " 'zabardast',\n",
       " 'anoth',\n",
       " 'atb',\n",
       " \"'s\",\n",
       " 'way',\n",
       " '!',\n",
       " 'i',\n",
       " 'wish',\n",
       " 'srk',\n",
       " 'sir',\n",
       " 'start',\n",
       " 'sign',\n",
       " 'good',\n",
       " 'movy',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'want',\n",
       " 'jab',\n",
       " 'cry',\n",
       " 'sociopa',\n",
       " 'ful',\n",
       " 'raid',\n",
       " 'gear',\n",
       " 'sad',\n",
       " 'when',\n",
       " 'say',\n",
       " 'hi',\n",
       " 'sunshin',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'feel',\n",
       " 'bad',\n",
       " 'ahah',\n",
       " 'unhappy',\n",
       " \"'s\",\n",
       " 'get',\n",
       " 'hard',\n",
       " 'hard',\n",
       " 'stay',\n",
       " 'unhappy',\n",
       " 'his',\n",
       " 'fac',\n",
       " 'look',\n",
       " 'blo',\n",
       " 'unhappy',\n",
       " 'baby',\n",
       " 'get',\n",
       " 'wel',\n",
       " 'soon',\n",
       " 'fuck',\n",
       " '.',\n",
       " 'tri',\n",
       " 'chang',\n",
       " 'set',\n",
       " 'stil',\n",
       " 'ind',\n",
       " '.',\n",
       " 'unhappy',\n",
       " 'talk',\n",
       " 'driv',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'goingh',\n",
       " 'said',\n",
       " \"'d\",\n",
       " 'lov',\n",
       " 'go',\n",
       " 'new',\n",
       " 'york',\n",
       " 'sint',\n",
       " 'trump',\n",
       " \"'s\",\n",
       " 'prob',\n",
       " 'not.1',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'forget',\n",
       " \"'s\",\n",
       " 'also',\n",
       " 'gabriel',\n",
       " 'tenm',\n",
       " 'whit',\n",
       " \"'s\",\n",
       " 'birthday',\n",
       " 'today',\n",
       " '!',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'unhappy',\n",
       " 'why',\n",
       " 'alway',\n",
       " 'tak',\n",
       " 'grant',\n",
       " 'eversint',\n",
       " 'unhappy',\n",
       " 'ah',\n",
       " 'alright',\n",
       " '%',\n",
       " '27t',\n",
       " 'know',\n",
       " 'saw',\n",
       " 'com',\n",
       " 'yet',\n",
       " 'camer',\n",
       " 'shoot',\n",
       " 'flip',\n",
       " 'scr',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'lou',\n",
       " \"'\",\n",
       " 'tweet',\n",
       " 'unhappy',\n",
       " 'koala',\n",
       " 'dying',\n",
       " 'thirst',\n",
       " \"'s\",\n",
       " 'us',\n",
       " 'unhappy',\n",
       " 'okay',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'shut',\n",
       " '.',\n",
       " 'inst',\n",
       " 'mess',\n",
       " 'mad',\n",
       " 'lot',\n",
       " 'peopl',\n",
       " 'very',\n",
       " 'flaw',\n",
       " 'opin',\n",
       " 'ment',\n",
       " 'heal',\n",
       " '(',\n",
       " 'min',\n",
       " ')',\n",
       " 'show',\n",
       " 'unhappy',\n",
       " 'pamur',\n",
       " 'is',\n",
       " 'mom',\n",
       " 'want',\n",
       " 'explod',\n",
       " 'lik',\n",
       " 'grenad',\n",
       " 'point',\n",
       " 'peopl',\n",
       " 'die',\n",
       " '.',\n",
       " 'sad',\n",
       " 'yg',\n",
       " 'sent',\n",
       " 'mcd',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'troph',\n",
       " 'unhappy',\n",
       " 'anyway',\n",
       " 'real',\n",
       " 'want',\n",
       " 'on',\n",
       " 'icon',\n",
       " 'jimin',\n",
       " 'stripes',\n",
       " 'turtleneck',\n",
       " 'shirt',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'want',\n",
       " 'spoon',\n",
       " 'i',\n",
       " 'cant',\n",
       " 'go',\n",
       " 'unhappy',\n",
       " 'honest',\n",
       " 'feel',\n",
       " 'lik',\n",
       " 'messy',\n",
       " 'break',\n",
       " 'unhappy',\n",
       " 'mak',\n",
       " 'sad',\n",
       " 'unhappy',\n",
       " 'look',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'went',\n",
       " 'seaworld',\n",
       " 'yg',\n",
       " 'sent',\n",
       " 'mcd',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'troph',\n",
       " 'unhappy',\n",
       " 'anyway',\n",
       " '.1',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'rocky',\n",
       " 'post',\n",
       " 'unhappy',\n",
       " 'hey',\n",
       " 'tony',\n",
       " 'oh',\n",
       " 'unhappy',\n",
       " 'could',\n",
       " 'pleas',\n",
       " 'tel',\n",
       " 'littl',\n",
       " 'issu',\n",
       " '?',\n",
       " 'im',\n",
       " 'follow',\n",
       " 'youd',\n",
       " 'pref',\n",
       " 'dm',\n",
       " '.',\n",
       " 'amand',\n",
       " 'lov',\n",
       " 'mason',\n",
       " 'miss',\n",
       " 'mason',\n",
       " 'unhappy',\n",
       " 'cold',\n",
       " 'moth',\n",
       " 'crush',\n",
       " 'right',\n",
       " '.',\n",
       " 'near',\n",
       " 'end',\n",
       " 'april',\n",
       " '.',\n",
       " 'sad',\n",
       " '%',\n",
       " '27t',\n",
       " 'talk',\n",
       " 'anym',\n",
       " 'lik',\n",
       " 'us',\n",
       " 'unhappy',\n",
       " 'yg',\n",
       " 'sent',\n",
       " 'mcd',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'troph',\n",
       " 'unhappy',\n",
       " 'anyway',\n",
       " '.2',\n",
       " 'miss',\n",
       " 'bik',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'big',\n",
       " 'broth',\n",
       " 'unhappy',\n",
       " '6',\n",
       " 'day',\n",
       " 'camp',\n",
       " 'haiss',\n",
       " 'miss',\n",
       " 'lot',\n",
       " 'unhappy',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'rain',\n",
       " 'hard',\n",
       " 'unhappy',\n",
       " 'am',\n",
       " 'bor',\n",
       " 'kandowiandg',\n",
       " 'i',\n",
       " 'ando',\n",
       " 'plaand',\n",
       " 'today',\n",
       " 'makiandg',\n",
       " 'eveand',\n",
       " 'bor',\n",
       " 'unhappy',\n",
       " 'oh',\n",
       " 'god',\n",
       " 'laury',\n",
       " 'penny',\n",
       " 'unhappy',\n",
       " 'say',\n",
       " 'hi',\n",
       " 'mekish',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'nev',\n",
       " 'draw',\n",
       " 'unhappy',\n",
       " 'cleanth',\n",
       " 'vis',\n",
       " 'studio',\n",
       " 'instal',\n",
       " '-',\n",
       " '89',\n",
       " '%',\n",
       " '..',\n",
       " 'bsod',\n",
       " 'com',\n",
       " 'sud',\n",
       " 'unhappy',\n",
       " 'want',\n",
       " 'mak',\n",
       " 'waffl',\n",
       " 'unhappy',\n",
       " 'so',\n",
       " 'sad',\n",
       " 'unhappy',\n",
       " 'cry',\n",
       " 'muh',\n",
       " 'feel',\n",
       " 'look',\n",
       " 'lik',\n",
       " 'someth',\n",
       " 'ign',\n",
       " 'xd',\n",
       " 'kart',\n",
       " 'rac',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'at',\n",
       " 'jenn',\n",
       " 'block',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'my',\n",
       " 'bed',\n",
       " 'comfort',\n",
       " 'i',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'get',\n",
       " 'unhappy',\n",
       " 'is',\n",
       " 'stor',\n",
       " 'stil',\n",
       " 'us',\n",
       " '?',\n",
       " 'if',\n",
       " 'i',\n",
       " 'sint',\n",
       " 'hop',\n",
       " 'many',\n",
       " 'priceless',\n",
       " 'ant',\n",
       " 'destroy',\n",
       " '.',\n",
       " 'reg',\n",
       " 'astag',\n",
       " 'unhappy',\n",
       " '/',\n",
       " '?',\n",
       " 'i',\n",
       " 'want',\n",
       " 'puppy',\n",
       " 'unhappy',\n",
       " 'work',\n",
       " 'unhappy',\n",
       " \"'ll\",\n",
       " 'see',\n",
       " 'tomorrow',\n",
       " '!',\n",
       " '!',\n",
       " 'happy',\n",
       " 'wee',\n",
       " 'day',\n",
       " 'without',\n",
       " 'anym',\n",
       " 'unhappy',\n",
       " 'favourit',\n",
       " 'lipstick',\n",
       " 'hilang',\n",
       " 'cry',\n",
       " 'tim',\n",
       " 'fli',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'believ',\n",
       " 'year',\n",
       " 'next',\n",
       " 'year',\n",
       " 'unhappy',\n",
       " 'we',\n",
       " \"'re\",\n",
       " 'becom',\n",
       " 'old',\n",
       " 'hahahah',\n",
       " ':',\n",
       " 'v',\n",
       " 'the',\n",
       " 'new',\n",
       " 'twit',\n",
       " 'reply',\n",
       " 'view',\n",
       " 'confus',\n",
       " '...',\n",
       " 'lik',\n",
       " 'i',\n",
       " 'capit',\n",
       " 'reply',\n",
       " 'peopl',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'whaddup',\n",
       " '.',\n",
       " 'me',\n",
       " 'cry',\n",
       " 'unhappy',\n",
       " 'every',\n",
       " 'tim',\n",
       " 'laugh',\n",
       " 'ass',\n",
       " \"'s\",\n",
       " 'sel',\n",
       " 'army',\n",
       " 'bomb',\n",
       " 'ver',\n",
       " '2',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " 'meet',\n",
       " 'sad',\n",
       " 'yeesh',\n",
       " 'unhappy',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'fair',\n",
       " 'warm',\n",
       " '.',\n",
       " 'east',\n",
       " 'flown',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'ready',\n",
       " 'giv',\n",
       " 'hom',\n",
       " 'luxury',\n",
       " 'lik',\n",
       " 'brand',\n",
       " 'cer',\n",
       " 'ohnoo',\n",
       " 'unhappy',\n",
       " 'unhappy',\n",
       " 'hop',\n",
       " 'recup',\n",
       " 'soon',\n",
       " '!',\n",
       " '!',\n",
       " 'her',\n",
       " 'back',\n",
       " 'unhappy',\n",
       " 'giv',\n",
       " 'chant',\n",
       " 'west',\n",
       " 'serv',\n",
       " 'unhappy',\n",
       " 'going',\n",
       " 'yesterday',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'agr',\n",
       " '.',\n",
       " 'my',\n",
       " 'issu',\n",
       " 'would',\n",
       " 'paid',\n",
       " 'somehow',\n",
       " '.',\n",
       " 'i',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'see',\n",
       " 'numb',\n",
       " 'ad',\n",
       " '.',\n",
       " 'sad',\n",
       " 'i',\n",
       " 'want',\n",
       " 'drink',\n",
       " 'cigaret',\n",
       " 'unhappy',\n",
       " 'oh',\n",
       " 'mint',\n",
       " 'unhappy',\n",
       " 'the',\n",
       " 'manifesto',\n",
       " 'nick',\n",
       " '?',\n",
       " 'might',\n",
       " 'del',\n",
       " 'when',\n",
       " 'tim',\n",
       " 'demand',\n",
       " '.',\n",
       " 'story',\n",
       " 'lif',\n",
       " '-',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'okay',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'acceiv',\n",
       " 'way',\n",
       " 'unhappy',\n",
       " 'peopl',\n",
       " 'abus',\n",
       " 'anim',\n",
       " 'unhappy',\n",
       " \"'re\",\n",
       " 'loy',\n",
       " \"'m\",\n",
       " 'act',\n",
       " 'cry',\n",
       " 'typ',\n",
       " 'tweet',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'tak',\n",
       " 'anym',\n",
       " '..',\n",
       " 'applicablehulog',\n",
       " 'appl',\n",
       " 'knock',\n",
       " 'unhappy',\n",
       " 'imagin',\n",
       " 'win',\n",
       " 'next',\n",
       " 'tim',\n",
       " 'unhappy',\n",
       " 'unhappy',\n",
       " 'sam',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'nee',\n",
       " 'cue',\n",
       " 'someth',\n",
       " 'would',\n",
       " 'mak',\n",
       " 'smil',\n",
       " '....',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'wait',\n",
       " '...',\n",
       " ':',\n",
       " '(',\n",
       " 'yg',\n",
       " 'sent',\n",
       " 'mcd',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'troph',\n",
       " 'unhappy',\n",
       " 'anyway',\n",
       " '.3',\n",
       " 'yg',\n",
       " 'sent',\n",
       " 'mcd',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'troph',\n",
       " 'unhappy',\n",
       " 'anyway',\n",
       " '.4',\n",
       " 'noseblee',\n",
       " 'get',\n",
       " 'outt',\n",
       " 'hand',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'perfect',\n",
       " 'happy',\n",
       " 'singl',\n",
       " '..',\n",
       " 'until',\n",
       " 'i',\n",
       " 'see',\n",
       " 'happy',\n",
       " 'coupl',\n",
       " ':',\n",
       " '(',\n",
       " 'kiss',\n",
       " 'thefashionicon',\n",
       " 'dud',\n",
       " 'i',\n",
       " 'want',\n",
       " 'sleep',\n",
       " 'unhappy',\n",
       " 'feel',\n",
       " 'fuck',\n",
       " 'shit',\n",
       " 'today',\n",
       " 'unhappy',\n",
       " 'wont',\n",
       " 'abl',\n",
       " 'stream',\n",
       " 'tonight',\n",
       " \"'m\",\n",
       " 'sorry',\n",
       " 'guy',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'fac',\n",
       " 'swap',\n",
       " 'cat',\n",
       " 'dog',\n",
       " \"'s\",\n",
       " 'real',\n",
       " 'upset',\n",
       " 'unhappy',\n",
       " 'system',\n",
       " 'recogn',\n",
       " 'spac',\n",
       " 'last',\n",
       " 'nam',\n",
       " '2nd',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df = pd.read_csv('data/processedNegative.csv').T.reset_index()\n",
    "neg_text = \" \".join([tweet[0] for tweet in neg_df.values.tolist()])\n",
    "neg_tokens = [word for word in word_tokenize(neg_text) if not word in stopwords.words('english')]\n",
    "ls = LancasterStemmer()\n",
    "neg_stem = [ls.stem(word) for word in neg_tokens]\n",
    "neg_stem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Функция, которая создасть набор данных для обучения моделей<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_file_to_df(file_name):\n",
    "    neg_fn, neut_fn, pos_fn = file_name\n",
    "\n",
    "    neg_df = pd.read_csv(neg_fn).T.reset_index()\n",
    "    neut_df = pd.read_csv(neut_fn).T.reset_index()\n",
    "    pos_df = pd.read_csv(pos_fn).T.reset_index()\n",
    "    \n",
    "    neg_text = \" \".join([tweet[0] for tweet in neg_df.values.tolist()])\n",
    "    neut_text = \" \".join([tweet[0] for tweet in neut_df.values.tolist()])\n",
    "    pos_text = \" \".join([tweet[0] for tweet in pos_df.values.tolist()])\n",
    "\n",
    "    ls = LancasterStemmer()\n",
    "    \n",
    "    neg_words = Counter([ls.stem(word) for word in word_tokenize(neg_text) if not word in stopwords.words('english')])\n",
    "    neut_words = Counter([ls.stem(word) for word in word_tokenize(neut_text) if not word in stopwords.words('english')])\n",
    "    pos_words = Counter([ls.stem(word) for word in word_tokenize(pos_text) if not word in stopwords.words('english')])\n",
    "    \n",
    "    unic_words = list(set(neg_words.keys()) | set(neut_words.keys()) | set(pos_words.keys()))\n",
    "\n",
    "    neg_exist_index = 0\n",
    "    neut_exist_index = 1\n",
    "    pos_exist_index = 2\n",
    "    neg_count_index = 3\n",
    "    neut_count_index = 4\n",
    "    pos_count_index = 5\n",
    "    word_count_index = 6\n",
    "    neg_tfidf_index = 7\n",
    "    neut_tfidf_index = 8\n",
    "    pos_tfidf_index = 9\n",
    "\n",
    "    df = np.zeros((len(unic_words), 10))\n",
    "    for i, word in enumerate(unic_words):\n",
    "        if word in neg_words.keys():\n",
    "            df[i,neg_exist_index] = 1\n",
    "            df[i,neg_count_index] = neg_words[word]\n",
    "        if word in neut_words.keys():\n",
    "            df[i,neut_exist_index] = 1\n",
    "            df[i,neut_count_index] = neut_words[word]\n",
    "        if word in pos_words.keys():\n",
    "            df[i,pos_exist_index] = 1\n",
    "            df[i,pos_count_index] = pos_words[word]\n",
    "\n",
    "    df[:,word_count_index] = df[:,neg_count_index] + df[:,neut_count_index] + df[:,pos_count_index]\n",
    "    df[:,neg_tfidf_index] = df[:,neg_count_index] / df[:,word_count_index]\n",
    "    df[:,neut_tfidf_index] = df[:,neut_count_index] / df[:,word_count_index]\n",
    "    df[:,pos_tfidf_index] = df[:,pos_count_index] / df[:,word_count_index]\n",
    "\n",
    "    stem_df = pd.DataFrame(df, columns=[\n",
    "        'Negative', 'Neutral', 'Positive',\n",
    "        'Negative counts', 'Neutral counts', 'Positive counts', 'Word counts',\n",
    "        'Negative TFIDF', 'Neutral TFIDF', 'Positive TFIDF'])\n",
    "    stem_df[\"word\"] = unic_words\n",
    "    return stem_df, unic_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Узнаем, как называются остальные файлы, содержащие исходный набор данных<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mmisspel.csv\u001b[0m*     \u001b[01;32mprocessedNegative.csv\u001b[0m*  \u001b[01;32mprocessedPositive.csv\u001b[0m*\n",
      "\u001b[01;32mp00_tweets.zip\u001b[0m*  \u001b[01;32mprocessedNeutral.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Создадим набор данных для обучения<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative counts</th>\n",
       "      <th>Neutral counts</th>\n",
       "      <th>Positive counts</th>\n",
       "      <th>Word counts</th>\n",
       "      <th>Negative TFIDF</th>\n",
       "      <th>Neutral TFIDF</th>\n",
       "      <th>Positive TFIDF</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>remov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>tor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mushroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>dynam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5041</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>guj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5042</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>edappad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5043</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>minecraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5044</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>standoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5045</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5046 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Negative  Neutral  Positive  Negative counts  Neutral counts  \\\n",
       "0          0.0      1.0       0.0              0.0             1.0   \n",
       "1          1.0      0.0       0.0              1.0             0.0   \n",
       "2          1.0      0.0       0.0              1.0             0.0   \n",
       "3          0.0      1.0       0.0              0.0             1.0   \n",
       "4          0.0      1.0       1.0              0.0             6.0   \n",
       "...        ...      ...       ...              ...             ...   \n",
       "5041       0.0      1.0       0.0              0.0             2.0   \n",
       "5042       0.0      1.0       0.0              0.0             3.0   \n",
       "5043       0.0      0.0       1.0              0.0             0.0   \n",
       "5044       0.0      1.0       0.0              0.0             1.0   \n",
       "5045       0.0      1.0       0.0              0.0             1.0   \n",
       "\n",
       "      Positive counts  Word counts  Negative TFIDF  Neutral TFIDF  \\\n",
       "0                 0.0          1.0             0.0           1.00   \n",
       "1                 0.0          1.0             1.0           0.00   \n",
       "2                 0.0          1.0             1.0           0.00   \n",
       "3                 0.0          1.0             0.0           1.00   \n",
       "4                 2.0          8.0             0.0           0.75   \n",
       "...               ...          ...             ...            ...   \n",
       "5041              0.0          2.0             0.0           1.00   \n",
       "5042              0.0          3.0             0.0           1.00   \n",
       "5043              2.0          2.0             0.0           0.00   \n",
       "5044              0.0          1.0             0.0           1.00   \n",
       "5045              0.0          1.0             0.0           1.00   \n",
       "\n",
       "      Positive TFIDF       word  \n",
       "0               0.00      remov  \n",
       "1               0.00        tor  \n",
       "2               0.00   mushroom  \n",
       "3               0.00      dynam  \n",
       "4               0.25    company  \n",
       "...              ...        ...  \n",
       "5041            0.00        guj  \n",
       "5042            0.00    edappad  \n",
       "5043            1.00  minecraft  \n",
       "5044            0.00   standoff  \n",
       "5045            0.00       1478  \n",
       "\n",
       "[5046 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = ('data/processedNegative.csv', 'data/processedNeutral.csv', 'data/processedPositive.csv')\n",
    "stem_df, unic_words = stem_file_to_df(file_names)\n",
    "stem_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Узнаем полученную точность модели<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "word_exist_accuracy_score = model_selection_word_exist(stem_df, unic_words)\n",
    "word_count_accuracy_score = model_selection_word_count(stem_df, unic_words)\n",
    "tfidf_accuracy_score = model_selection_tfidf(stem_df, unic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score by word exist: 0.4871287128712871\n",
      "Accuracy score by word count: 0.9544554455445544\n",
      "Fccuracy score by tfidf: 0.49306930693069306\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Accuracy score by word exist: {word_exist_accuracy_score}\n",
    "Accuracy score by word count: {word_count_accuracy_score}\n",
    "Fccuracy score by tfidf: {tfidf_accuracy_score}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
