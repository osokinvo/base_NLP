{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Опечатки и лематизация<h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Импортируем библиотеки<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/val/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/val/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/val/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "from collections import Counter\n",
    "from model_selction import model_selection_word_count, model_selection_word_exist, model_selection_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mp00_tweets.zip\u001b[0m*         \u001b[01;32mprocessedNeutral.csv\u001b[0m*\n",
      "\u001b[01;32mprocessedNegative.csv\u001b[0m*  \u001b[01;32mprocessedPositive.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>В качестве примера рассмотрим содержимое файла 'processedNegative.csv' после применения метода<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ho',\n",
       " 'unhappy',\n",
       " 'dog',\n",
       " 'like',\n",
       " 'though',\n",
       " 'talk',\n",
       " 'driver',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'go',\n",
       " 'say',\n",
       " \"'d\",\n",
       " 'love',\n",
       " 'go',\n",
       " 'Newar',\n",
       " 'Yorker',\n",
       " 'since',\n",
       " 'Triumph',\n",
       " \"'s\",\n",
       " 'probably',\n",
       " 'Doeg',\n",
       " 'anybody',\n",
       " 'know',\n",
       " 'Rand',\n",
       " \"'s\",\n",
       " 'likely',\n",
       " 'fall',\n",
       " 'dollar',\n",
       " 'I',\n",
       " 'get',\n",
       " 'money',\n",
       " 'I',\n",
       " 'need',\n",
       " 'change',\n",
       " 'keep',\n",
       " 'get',\n",
       " 'strong',\n",
       " 'unhappy',\n",
       " '?',\n",
       " 'R',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'go',\n",
       " 'gig',\n",
       " 'Liverpudlian',\n",
       " 'unhappy',\n",
       " 'Theresa',\n",
       " 'new',\n",
       " 'Ricciales',\n",
       " 'tonight',\n",
       " 'unhappy',\n",
       " '?',\n",
       " \"'s\",\n",
       " 'A',\n",
       " 'dye',\n",
       " 'guy',\n",
       " 'pop',\n",
       " 'Asian',\n",
       " 'translator',\n",
       " \"'ll\",\n",
       " 'prob',\n",
       " 'go',\n",
       " 'around',\n",
       " 'Aus',\n",
       " 'unhappy',\n",
       " '*',\n",
       " 'Whig',\n",
       " \"'s\",\n",
       " 'chair',\n",
       " 'sit',\n",
       " 'Isis',\n",
       " 'I',\n",
       " 'find',\n",
       " 'Everyman',\n",
       " 'know',\n",
       " 'Yomud',\n",
       " \"'ve\",\n",
       " 'sham',\n",
       " 'pu',\n",
       " \"'re\",\n",
       " '?',\n",
       " '.',\n",
       " '.',\n",
       " 'n',\n",
       " 'like',\n",
       " 'jittery',\n",
       " 'caffeine',\n",
       " 'make',\n",
       " 'sad',\n",
       " 'Mya',\n",
       " 'area',\n",
       " \"'s\",\n",
       " 'list',\n",
       " 'unhappy',\n",
       " 'think',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'go',\n",
       " 'Libby',\n",
       " 'anyway',\n",
       " 'I',\n",
       " 'want',\n",
       " 'fun',\n",
       " 'plan',\n",
       " 'weekend',\n",
       " 'unhappy',\n",
       " 'Wend',\n",
       " 'notice',\n",
       " 'unhappy',\n",
       " '.',\n",
       " '?',\n",
       " 'Ah',\n",
       " 'Yomud',\n",
       " 'recognize',\n",
       " 'L',\n",
       " 'Cinemascope',\n",
       " 'show',\n",
       " 'B',\n",
       " 'track',\n",
       " 'record',\n",
       " 'get',\n",
       " 'canceler',\n",
       " 'unhappy',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'Errantia',\n",
       " 'dude',\n",
       " 'The',\n",
       " 'go',\n",
       " 'unhappy',\n",
       " 'Askr',\n",
       " 'league',\n",
       " 'member',\n",
       " 'check',\n",
       " 'guy',\n",
       " 'go',\n",
       " '....',\n",
       " \"'re\",\n",
       " 'No',\n",
       " 'sad',\n",
       " 'Whig',\n",
       " 'would',\n",
       " 'Harvey',\n",
       " 'go',\n",
       " 'prison',\n",
       " 'unhappy',\n",
       " '?',\n",
       " 'Ming',\n",
       " 'cry',\n",
       " 'Sepioidea',\n",
       " 'area',\n",
       " '.',\n",
       " 'Beck',\n",
       " 'depend',\n",
       " 'promotion',\n",
       " 'waste',\n",
       " 'handwork',\n",
       " 'team',\n",
       " 'I',\n",
       " 'think',\n",
       " \"'ll\",\n",
       " 'save',\n",
       " 'cry',\n",
       " 'major',\n",
       " 'waffle',\n",
       " 'crave',\n",
       " 'right',\n",
       " 'sad',\n",
       " 'cant',\n",
       " 'speak',\n",
       " 'japan',\n",
       " ':',\n",
       " ':',\n",
       " '(',\n",
       " 'people',\n",
       " 'stuff',\n",
       " 'like',\n",
       " 'unhappy',\n",
       " 'please',\n",
       " 'stop',\n",
       " 'confine',\n",
       " 'animal',\n",
       " 'zoo',\n",
       " 'unhappy',\n",
       " 'Felis',\n",
       " 'like',\n",
       " 'tell',\n",
       " 'get',\n",
       " 'fucus',\n",
       " 'social',\n",
       " 'medium',\n",
       " 'byous',\n",
       " 'also',\n",
       " 'feel',\n",
       " 'really',\n",
       " 'mean',\n",
       " 'unhappy',\n",
       " 'silence',\n",
       " 'love',\n",
       " 'hope',\n",
       " 'oka',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'huh',\n",
       " 'busy',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'extend',\n",
       " 'family',\n",
       " 'popple',\n",
       " 'wanter',\n",
       " 'show',\n",
       " 'Ohio',\n",
       " 'Mya',\n",
       " 'Gi',\n",
       " 'dor',\n",
       " 'playingly',\n",
       " 'game',\n",
       " 'get',\n",
       " 'delete',\n",
       " 'unhappy',\n",
       " '.',\n",
       " '12',\n",
       " 'Dob',\n",
       " 'n',\n",
       " 'unhappy',\n",
       " 'Jamie',\n",
       " 'please',\n",
       " 'reset',\n",
       " 'C',\n",
       " 'grandfilial',\n",
       " 'serve',\n",
       " 'administrator',\n",
       " 'respond',\n",
       " 'unhappy',\n",
       " '...',\n",
       " 'nook',\n",
       " 'Y',\n",
       " 'G',\n",
       " 'N',\n",
       " 'M',\n",
       " 'T',\n",
       " 'B',\n",
       " 'unhappy',\n",
       " 'T',\n",
       " 'C',\n",
       " 'A',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'I',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'vote',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'instant',\n",
       " 'message',\n",
       " 'jealous',\n",
       " 'oka',\n",
       " 'unhappy',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'hah',\n",
       " 'brunt',\n",
       " 'wait',\n",
       " 'final',\n",
       " 'first',\n",
       " \"'m\",\n",
       " 'enlist',\n",
       " 'please',\n",
       " 'turn',\n",
       " 'like',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'unhappy',\n",
       " 'come',\n",
       " 'people',\n",
       " 'like',\n",
       " 'childrenite',\n",
       " \"'s\",\n",
       " 'state',\n",
       " 'intervention',\n",
       " 'Ophiuchid',\n",
       " 'unhappy',\n",
       " 'Helen',\n",
       " 'I',\n",
       " 'want',\n",
       " 'stop',\n",
       " 'teeting',\n",
       " 'Alle',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'endless',\n",
       " 'suffering',\n",
       " 'pain',\n",
       " 'I',\n",
       " 'try',\n",
       " 'deactivate',\n",
       " 'many',\n",
       " 'time',\n",
       " 'Savery',\n",
       " 'unhappy',\n",
       " '...',\n",
       " '.',\n",
       " '.',\n",
       " '...',\n",
       " '...',\n",
       " 'For',\n",
       " 'askingly',\n",
       " 'application',\n",
       " 'Kanaka',\n",
       " 'Kanji',\n",
       " 'Funtumia',\n",
       " 'Saad',\n",
       " '!',\n",
       " 'look',\n",
       " 'like',\n",
       " 'due',\n",
       " 'kill',\n",
       " 'unhappy',\n",
       " '11',\n",
       " 'Yeshibah',\n",
       " 'update',\n",
       " '16.04',\n",
       " 'freeze',\n",
       " 'time',\n",
       " 'The',\n",
       " 'go',\n",
       " '.',\n",
       " '16.10',\n",
       " 'froze',\n",
       " 'mid',\n",
       " 'install',\n",
       " 'Wagnerite',\n",
       " \"'d\",\n",
       " 'pull',\n",
       " 'plug',\n",
       " 'cry',\n",
       " '.',\n",
       " '3hrs',\n",
       " 'Shardana',\n",
       " 'Zabaean',\n",
       " 'Anoplotherium',\n",
       " 'A',\n",
       " \"'s\",\n",
       " 'way',\n",
       " 'I',\n",
       " 'wish',\n",
       " 'Sri',\n",
       " 'Sir',\n",
       " 'start',\n",
       " 'sigh',\n",
       " 'good',\n",
       " 'movie',\n",
       " 'unhappy',\n",
       " '!',\n",
       " 'I',\n",
       " 'want',\n",
       " 'Jabberwock',\n",
       " 'cry',\n",
       " 'sociocrat',\n",
       " 'full',\n",
       " 'raid',\n",
       " 'gear',\n",
       " 'sad',\n",
       " 'Wend',\n",
       " 'say',\n",
       " 'hi',\n",
       " 'sunshine',\n",
       " 'unhappy',\n",
       " '?',\n",
       " 'feel',\n",
       " 'bad',\n",
       " 'A',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " \"'s\",\n",
       " 'get',\n",
       " 'hard',\n",
       " 'harder',\n",
       " 'stay',\n",
       " 'unhappy',\n",
       " 'Hispa',\n",
       " 'face',\n",
       " 'look',\n",
       " 'bloat',\n",
       " 'unhappy',\n",
       " 'baby',\n",
       " 'get',\n",
       " 'well',\n",
       " 'soon',\n",
       " 'fucus',\n",
       " 'try',\n",
       " 'chang',\n",
       " 'setting',\n",
       " 'still',\n",
       " 'indicia',\n",
       " 'unhappy',\n",
       " '.',\n",
       " '.',\n",
       " 'talk',\n",
       " 'driver',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'go',\n",
       " 'say',\n",
       " \"'d\",\n",
       " 'love',\n",
       " 'go',\n",
       " 'Newar',\n",
       " 'Yorker',\n",
       " 'since',\n",
       " 'Triumph',\n",
       " \"'s\",\n",
       " 'probably',\n",
       " 'Leto',\n",
       " \"'s\",\n",
       " 'forget',\n",
       " \"'s\",\n",
       " 'also',\n",
       " 'Gabriel',\n",
       " 'Tema',\n",
       " 'Whit',\n",
       " \"'s\",\n",
       " 'birthday',\n",
       " 'today',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'unhappy',\n",
       " '!',\n",
       " 'Whig',\n",
       " 'always',\n",
       " 'take',\n",
       " 'grantedly',\n",
       " 'eversive',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'Ah',\n",
       " 'alright',\n",
       " 'know',\n",
       " 'saw',\n",
       " 'comment',\n",
       " 'yet',\n",
       " 'i',\n",
       " '%',\n",
       " '27t',\n",
       " 'camera',\n",
       " 'shoot',\n",
       " 'flip',\n",
       " 'scry',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'Louis',\n",
       " 'tweet',\n",
       " 'unhappy',\n",
       " \"'\",\n",
       " 'Koasati',\n",
       " 'die',\n",
       " 'thirst',\n",
       " \"'s\",\n",
       " 'u',\n",
       " 'unhappy',\n",
       " 'oka',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'shut',\n",
       " 'instant',\n",
       " 'message',\n",
       " 'mad',\n",
       " 'lot',\n",
       " 'people',\n",
       " 'V',\n",
       " 'flaw',\n",
       " 'opinion',\n",
       " 'mental',\n",
       " 'health',\n",
       " 'mine',\n",
       " 'show',\n",
       " 'unhappy',\n",
       " '.',\n",
       " '(',\n",
       " ')',\n",
       " 'param',\n",
       " 'moment',\n",
       " 'want',\n",
       " 'explode',\n",
       " 'like',\n",
       " 'grenade',\n",
       " 'point',\n",
       " 'people',\n",
       " 'die',\n",
       " 'sad',\n",
       " 'i',\n",
       " '.',\n",
       " 'Y',\n",
       " 'send',\n",
       " 'M',\n",
       " 'I',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'trophy',\n",
       " 'unhappy',\n",
       " 'anyways',\n",
       " '.',\n",
       " 'really',\n",
       " 'want',\n",
       " 'one',\n",
       " 'iconic',\n",
       " 'jimp',\n",
       " 'stripe',\n",
       " 'turtle',\n",
       " 'shirt',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'I',\n",
       " 'want',\n",
       " 'spoon',\n",
       " 'I',\n",
       " 'cant',\n",
       " 'go',\n",
       " 'unhappy',\n",
       " 'honestly',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'messy',\n",
       " 'break',\n",
       " 'unhappy',\n",
       " 'Maku',\n",
       " 'sad',\n",
       " 'unhappy',\n",
       " 'looker',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'go',\n",
       " 'Sean',\n",
       " 'Y',\n",
       " 'send',\n",
       " 'M',\n",
       " 'I',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'trophy',\n",
       " 'unhappy',\n",
       " 'anyways',\n",
       " '.',\n",
       " '.1',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'Rockies',\n",
       " 'post',\n",
       " 'unhappy',\n",
       " 'Hehe',\n",
       " 'Tony',\n",
       " 'oh',\n",
       " 'unhappy',\n",
       " 'Cours',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'little',\n",
       " 'issue',\n",
       " 'Ima',\n",
       " 'follow',\n",
       " 'youd',\n",
       " 'prefer',\n",
       " 'D',\n",
       " 'Amanda',\n",
       " '?',\n",
       " '.',\n",
       " 'love',\n",
       " 'mason',\n",
       " 'miss',\n",
       " 'mason',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'Cola',\n",
       " 'mother',\n",
       " 'crusher',\n",
       " 'right',\n",
       " 'Nerine',\n",
       " 'end',\n",
       " 'April',\n",
       " 'sad',\n",
       " '.',\n",
       " '.',\n",
       " 'talk',\n",
       " 'ananym',\n",
       " 'like',\n",
       " 'use',\n",
       " 'unhappy',\n",
       " '%',\n",
       " '27t',\n",
       " 'Y',\n",
       " 'send',\n",
       " 'M',\n",
       " 'I',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'trophy',\n",
       " 'unhappy',\n",
       " 'anyways',\n",
       " '.',\n",
       " '.2',\n",
       " 'miss',\n",
       " 'bikini',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'Bim',\n",
       " 'brother',\n",
       " 'unhappy',\n",
       " 'day',\n",
       " 'camp',\n",
       " 'haik',\n",
       " 'miss',\n",
       " 'lot',\n",
       " 'unhappy',\n",
       " '6',\n",
       " 'Ita',\n",
       " \"'s\",\n",
       " 'rain',\n",
       " 'hard',\n",
       " 'unhappy',\n",
       " 'Ami',\n",
       " 'bore',\n",
       " 'kandol',\n",
       " 'I',\n",
       " 'plan',\n",
       " 'today',\n",
       " 'manakin',\n",
       " 'ean',\n",
       " 'bore',\n",
       " 'unhappy',\n",
       " 'oh',\n",
       " 'god',\n",
       " 'lauric',\n",
       " 'penny',\n",
       " 'unhappy',\n",
       " 'say',\n",
       " 'Hima',\n",
       " 'Medish',\n",
       " 'unhappy',\n",
       " '?',\n",
       " 'I',\n",
       " 'never',\n",
       " 'draw',\n",
       " 'unhappy',\n",
       " 'Cyclanthales',\n",
       " 'Vishal',\n",
       " 'Studite',\n",
       " 'Italianation',\n",
       " 'B',\n",
       " 'come',\n",
       " 'suddenly',\n",
       " 'unhappy',\n",
       " '-',\n",
       " '89',\n",
       " '%',\n",
       " '..',\n",
       " 'want',\n",
       " 'make',\n",
       " 'waffle',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'Sol',\n",
       " 'sad',\n",
       " 'unhappy',\n",
       " 'cry',\n",
       " 'mu',\n",
       " 'feel',\n",
       " 'Lo',\n",
       " 'like',\n",
       " 'something',\n",
       " 'ignore',\n",
       " 'x',\n",
       " 'Kari',\n",
       " 'Rauraci',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " '?',\n",
       " 'eat',\n",
       " 'jenna',\n",
       " 'block',\n",
       " 'unhappy',\n",
       " '?',\n",
       " 'Mya',\n",
       " 'bed',\n",
       " 'comfortable',\n",
       " 'I',\n",
       " 'n',\n",
       " 'want',\n",
       " 'get',\n",
       " 'unhappy',\n",
       " 'Isis',\n",
       " 'store',\n",
       " 'still',\n",
       " 'use',\n",
       " 'Ifugao',\n",
       " '?',\n",
       " 'I',\n",
       " 'sincerely',\n",
       " 'hope',\n",
       " 'many',\n",
       " 'priceless',\n",
       " 'antique',\n",
       " 'destroyer',\n",
       " 'Regga',\n",
       " '.',\n",
       " 'Astragalus',\n",
       " 'unhappy',\n",
       " '/',\n",
       " '?',\n",
       " 'I',\n",
       " 'want',\n",
       " 'puppy',\n",
       " 'unhappy',\n",
       " 'work',\n",
       " 'unhappy',\n",
       " \"'ll\",\n",
       " 'see',\n",
       " 'tomorrow',\n",
       " 'i',\n",
       " '!',\n",
       " '!',\n",
       " 'happy',\n",
       " 'weed',\n",
       " 'day',\n",
       " 'without',\n",
       " 'ananym',\n",
       " 'unhappy',\n",
       " 'favorite',\n",
       " 'lipstick',\n",
       " 'hilding',\n",
       " 'cry',\n",
       " 'Tim',\n",
       " 'flier',\n",
       " 'ca',\n",
       " 'n',\n",
       " 'believe',\n",
       " 'year',\n",
       " 'next',\n",
       " 'year',\n",
       " 'unhappy',\n",
       " 'Wea',\n",
       " 'become',\n",
       " 'old',\n",
       " 'H',\n",
       " \"'re\",\n",
       " ':',\n",
       " 'v',\n",
       " 'The',\n",
       " 'new',\n",
       " 'Twi',\n",
       " 'reply',\n",
       " 'view',\n",
       " 'hash',\n",
       " 'confuse',\n",
       " 'like',\n",
       " 'I',\n",
       " 'capitalism',\n",
       " 'replier',\n",
       " 'people',\n",
       " 'unhappy',\n",
       " '...',\n",
       " '?',\n",
       " 'Whitechapel',\n",
       " 'Meg',\n",
       " 'cry',\n",
       " '.',\n",
       " 'unhappy',\n",
       " 'every',\n",
       " 'time',\n",
       " 'laugh',\n",
       " \"'s\",\n",
       " 'sell',\n",
       " 'army',\n",
       " 'bomb',\n",
       " 'veer',\n",
       " 'meet',\n",
       " 'sad',\n",
       " '2',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " 'Yeshiva',\n",
       " 'unhappy',\n",
       " 'Ita',\n",
       " \"'s\",\n",
       " 'fairly',\n",
       " 'warm',\n",
       " '.',\n",
       " 'easter',\n",
       " 'hash',\n",
       " 'flown',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'ready',\n",
       " 'give',\n",
       " 'home',\n",
       " 'luxurist',\n",
       " 'like',\n",
       " 'brand',\n",
       " 'cereal',\n",
       " 'Ohio',\n",
       " 'unhappy',\n",
       " 'unhappy',\n",
       " 'hope',\n",
       " 'recuperate',\n",
       " 'sooner',\n",
       " '!',\n",
       " '!',\n",
       " 'Herb',\n",
       " 'back',\n",
       " 'unhappy',\n",
       " 'give',\n",
       " 'chance',\n",
       " 'west',\n",
       " 'serve',\n",
       " 'unhappy',\n",
       " 'go',\n",
       " 'yesterday',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'agree',\n",
       " 'Mya',\n",
       " 'issue',\n",
       " 'would',\n",
       " 'hash',\n",
       " 'paik',\n",
       " 'somehow',\n",
       " 'I',\n",
       " 'ca',\n",
       " 'n',\n",
       " 'see',\n",
       " 'number',\n",
       " 'addle',\n",
       " 'sad',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'I',\n",
       " 'want',\n",
       " 'drink',\n",
       " 'cigarette',\n",
       " 'unhappy',\n",
       " 'Ohio',\n",
       " 'mince',\n",
       " 'unhappy',\n",
       " 'The',\n",
       " 'manifesto',\n",
       " 'Nick',\n",
       " 'Miguel',\n",
       " 'deliver',\n",
       " 'W',\n",
       " 'time',\n",
       " 'demand',\n",
       " 'Stentor',\n",
       " 'life',\n",
       " '?',\n",
       " '.',\n",
       " '-',\n",
       " 'Ita',\n",
       " \"'s\",\n",
       " 'oka',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'accept',\n",
       " 'way',\n",
       " 'unhappy',\n",
       " 'people',\n",
       " 'abuse',\n",
       " 'animal',\n",
       " 'unhappy',\n",
       " 'loyal',\n",
       " \"'re\",\n",
       " \"'m\",\n",
       " 'actually',\n",
       " 'cry',\n",
       " 'teletyping',\n",
       " 'tweet',\n",
       " 'ca',\n",
       " 'n',\n",
       " 'take',\n",
       " 'ananym',\n",
       " 'applicable',\n",
       " 'applicable',\n",
       " 'knock',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'i',\n",
       " '..',\n",
       " 'd',\n",
       " 'imagine',\n",
       " 'win',\n",
       " 'next',\n",
       " 'time',\n",
       " 'unhappy',\n",
       " 'unhappy',\n",
       " 'Sam',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'need',\n",
       " 'Cueva',\n",
       " 'something',\n",
       " 'would',\n",
       " 'make',\n",
       " 'smile',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'wait',\n",
       " '....',\n",
       " '...',\n",
       " ':',\n",
       " '(',\n",
       " 'Y',\n",
       " 'send',\n",
       " 'M',\n",
       " 'I',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'trophy',\n",
       " 'unhappy',\n",
       " 'anyways',\n",
       " '.',\n",
       " '.3',\n",
       " 'Y',\n",
       " 'send',\n",
       " 'M',\n",
       " 'I',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'trophy',\n",
       " 'unhappy',\n",
       " 'anyways',\n",
       " '.',\n",
       " '.4',\n",
       " 'nosebleed',\n",
       " 'get',\n",
       " 'outtalk',\n",
       " 'hand',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'perfectly',\n",
       " 'happy',\n",
       " 'single',\n",
       " 'Umatilla',\n",
       " 'I',\n",
       " 'see',\n",
       " 'happy',\n",
       " 'couple',\n",
       " 'K',\n",
       " 'Theridion',\n",
       " '..',\n",
       " ':',\n",
       " '(',\n",
       " 'Dudleya',\n",
       " 'I',\n",
       " 'want',\n",
       " 'sleep',\n",
       " 'unhappy',\n",
       " 'Felis',\n",
       " 'fackings',\n",
       " 'shita',\n",
       " 'today',\n",
       " 'unhappy',\n",
       " 'wont',\n",
       " 'able',\n",
       " 'stream',\n",
       " 'tonight',\n",
       " \"'m\",\n",
       " 'sorry',\n",
       " 'guy',\n",
       " 'unhappy',\n",
       " 'i',\n",
       " 'I',\n",
       " 'face',\n",
       " 'swapper',\n",
       " 'cat',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df = pd.read_csv('data/processedNegative.csv').T.reset_index()\n",
    "correct_words = words.words()\n",
    "wordnet = WordNetLemmatizer()\n",
    "neg_token = list()\n",
    "cor_text = list()\n",
    "lem_text = list()\n",
    "uncor_text = list()\n",
    "for sen in neg_df.values.tolist():\n",
    "    text = word_tokenize(sen[0])\n",
    "    cor_text.clear()\n",
    "    uncor_text.clear()\n",
    "    for i in range(len(text)):\n",
    "        word = text[i]\n",
    "        if word in [\"'m\", \"'ll\", \"'ve\", \"'d\", \"'s\"]:\n",
    "            cor_text.append(word)\n",
    "        elif len(word) > 1:\n",
    "            temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                                    set(ngrams(w, 2))),w)\n",
    "                    for w in correct_words if w[0]==word[0]]\n",
    "            try:\n",
    "                cor_text.append(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "            except IndexError:\n",
    "                uncor_text.append(word)\n",
    "        elif word in ['I', 'a', 'A']:\n",
    "            cor_text.append(word)\n",
    "        else:\n",
    "            uncor_text.append(word)\n",
    "    lem_text.clear()\n",
    "    for token, tag in pos_tag(cor_text):\n",
    "        pos = tag[0].lower()\n",
    "        if pos not in ['a', 'r', 'n', 'v']:\n",
    "            pos = 'n'\n",
    "        lem_text.append(wordnet.lemmatize(token, pos))\n",
    "    lem_text = [word for word in lem_text if not word in stopwords.words('english')]\n",
    "    neg_token.extend(lem_text)\n",
    "    neg_token.extend(uncor_text)\n",
    "neg_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Функция, которая создасть набор данных для обучения моделей<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_misspel_file_to_list(file_name):\n",
    "    df = pd.read_csv(file_name).T.reset_index()\n",
    "    correct_words = words.words()\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    tokens = list()\n",
    "    cor_text = list()\n",
    "    lem_text = list()\n",
    "    uncor_text = list()\n",
    "    for sen in df.values.tolist():\n",
    "        text = word_tokenize(sen[0])\n",
    "        cor_text.clear()\n",
    "        uncor_text.clear()\n",
    "        for i in range(len(text)):\n",
    "            word = text[i]\n",
    "            if word in [\"'m\", \"'ll\", \"'ve\", \"'d\", \"'s\"]:\n",
    "                cor_text.append(word)\n",
    "            elif len(word) > 1:\n",
    "                temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                                        set(ngrams(w, 2))),w)\n",
    "                        for w in correct_words if w[0]==word[0]]\n",
    "                try:\n",
    "                    cor_text.append(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "                except IndexError:\n",
    "                    uncor_text.append(word)\n",
    "            elif word in ['I', 'a', 'A']:\n",
    "                cor_text.append(word)\n",
    "            else:\n",
    "                uncor_text.append(word)\n",
    "        lem_text.clear()\n",
    "        for token, tag in pos_tag(cor_text):\n",
    "            pos = tag[0].lower()\n",
    "            if pos not in ['a', 'r', 'n', 'v']:\n",
    "                pos = 'n'\n",
    "            lem_text.append(wordnet.lemmatize(token, pos))\n",
    "        lem_text = [word for word in lem_text if not word in stopwords.words('english')]\n",
    "        tokens.extend(lem_text)\n",
    "        tokens.extend(uncor_text)\n",
    "    return tokens\n",
    "\n",
    "def lem_misspell_file_to_df(file_name):\n",
    "    neg_fn, neut_fn, pos_fn = file_name\n",
    "\n",
    "    neg_words = Counter(lem_misspel_file_to_list(neg_fn))\n",
    "    neut_words = Counter(lem_misspel_file_to_list(neut_fn))\n",
    "    pos_words = Counter(lem_misspel_file_to_list(pos_fn))\n",
    "    \n",
    "    unic_words = list(set(neg_words.keys()) | set(neut_words.keys()) | set(pos_words.keys()))\n",
    "\n",
    "    neg_exist_index = 0\n",
    "    neut_exist_index = 1\n",
    "    pos_exist_index = 2\n",
    "    neg_count_index = 3\n",
    "    neut_count_index = 4\n",
    "    pos_count_index = 5\n",
    "    word_count_index = 6\n",
    "    neg_tfidf_index = 7\n",
    "    neut_tfidf_index = 8\n",
    "    pos_tfidf_index = 9\n",
    "\n",
    "    df = np.zeros((len(unic_words), 10))\n",
    "    for i, word in enumerate(unic_words):\n",
    "        if word in neg_words.keys():\n",
    "            df[i,neg_exist_index] = 1\n",
    "            df[i,neg_count_index] = neg_words[word]\n",
    "        if word in neut_words.keys():\n",
    "            df[i,neut_exist_index] = 1\n",
    "            df[i,neut_count_index] = neut_words[word]\n",
    "        if word in pos_words.keys():\n",
    "            df[i,pos_exist_index] = 1\n",
    "            df[i,pos_count_index] = pos_words[word]\n",
    "\n",
    "    df[:,word_count_index] = df[:,neg_count_index] + df[:,neut_count_index] + df[:,pos_count_index]\n",
    "    df[:,neg_tfidf_index] = df[:,neg_count_index] / df[:,word_count_index]\n",
    "    df[:,neut_tfidf_index] = df[:,neut_count_index] / df[:,word_count_index]\n",
    "    df[:,pos_tfidf_index] = df[:,pos_count_index] / df[:,word_count_index]\n",
    "\n",
    "    spell_df = pd.DataFrame(df, columns=[\n",
    "        'Negative', 'Neutral', 'Positive',\n",
    "        'Negative counts', 'Neutral counts', 'Positive counts', 'Word counts',\n",
    "        'Negative TFIDF', 'Neutral TFIDF', 'Positive TFIDF'])\n",
    "    spell_df[\"word\"] = unic_words\n",
    "    return spell_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Узнаем, как называются остальные файлы, содержащие исходный набор данных<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mp00_tweets.zip\u001b[0m*         \u001b[01;32mprocessedNeutral.csv\u001b[0m*\n",
      "\u001b[01;32mprocessedNegative.csv\u001b[0m*  \u001b[01;32mprocessedPositive.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Создадим набор данных для обучения<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative counts</th>\n",
       "      <th>Neutral counts</th>\n",
       "      <th>Positive counts</th>\n",
       "      <th>Word counts</th>\n",
       "      <th>Negative TFIDF</th>\n",
       "      <th>Neutral TFIDF</th>\n",
       "      <th>Positive TFIDF</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Vern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Celeste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>affordable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5621</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Pondo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5623</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>hearing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5624</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Ricciales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5625</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>stage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5626 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Negative  Neutral  Positive  Negative counts  Neutral counts  \\\n",
       "0          1.0      0.0       1.0              3.0             0.0   \n",
       "1          1.0      1.0       1.0              1.0             1.0   \n",
       "2          1.0      0.0       1.0              1.0             0.0   \n",
       "3          0.0      1.0       0.0              0.0             1.0   \n",
       "4          1.0      1.0       1.0              1.0             1.0   \n",
       "...        ...      ...       ...              ...             ...   \n",
       "5621       0.0      1.0       0.0              0.0             2.0   \n",
       "5622       0.0      0.0       1.0              0.0             0.0   \n",
       "5623       0.0      1.0       0.0              0.0             1.0   \n",
       "5624       1.0      1.0       0.0              1.0             1.0   \n",
       "5625       1.0      1.0       0.0              1.0             1.0   \n",
       "\n",
       "      Positive counts  Word counts  Negative TFIDF  Neutral TFIDF  \\\n",
       "0                 2.0          5.0        0.600000       0.000000   \n",
       "1                 3.0          5.0        0.200000       0.200000   \n",
       "2                 1.0          2.0        0.500000       0.000000   \n",
       "3                 0.0          1.0        0.000000       1.000000   \n",
       "4                 7.0          9.0        0.111111       0.111111   \n",
       "...               ...          ...             ...            ...   \n",
       "5621              0.0          2.0        0.000000       1.000000   \n",
       "5622              1.0          1.0        0.000000       0.000000   \n",
       "5623              0.0          1.0        0.000000       1.000000   \n",
       "5624              0.0          2.0        0.500000       0.500000   \n",
       "5625              0.0          2.0        0.500000       0.500000   \n",
       "\n",
       "      Positive TFIDF         word  \n",
       "0           0.400000        sweet  \n",
       "1           0.600000         Vern  \n",
       "2           0.500000      Celeste  \n",
       "3           0.000000   affordable  \n",
       "4           0.777778  application  \n",
       "...              ...          ...  \n",
       "5621        0.000000         walk  \n",
       "5622        1.000000        Pondo  \n",
       "5623        0.000000      hearing  \n",
       "5624        0.000000    Ricciales  \n",
       "5625        0.000000        stage  \n",
       "\n",
       "[5626 rows x 11 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = ('data/processedNegative.csv', 'data/processedNeutral.csv', 'data/processedPositive.csv')\n",
    "lem_misspel_df = lem_misspell_file_to_df(file_names)\n",
    "lem_misspel_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Сохраним полученный набор данных, чтобы уменьшить ожидание при демонтсрации<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_misspel_df.to_csv('data/lem_misspel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_misspel_df = pd.read_csv('data/lem_misspel.csv')\n",
    "unic_words = lem_misspel_df.word.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sweet',\n",
       " 'Vern',\n",
       " 'Celeste',\n",
       " 'affordable',\n",
       " 'application',\n",
       " 'elder',\n",
       " 'sang',\n",
       " 'Hindu',\n",
       " 'guitar',\n",
       " 'Nigel',\n",
       " 'recommendee',\n",
       " 'criticism',\n",
       " 'Australia',\n",
       " 'cadet',\n",
       " 'notice',\n",
       " 'Secale',\n",
       " 'cheapery',\n",
       " 'Jayant',\n",
       " 'Lot',\n",
       " 'hi',\n",
       " 'calf',\n",
       " 'Eremian',\n",
       " 'monsterhood',\n",
       " 'December',\n",
       " 'deserve',\n",
       " 'already',\n",
       " 'Mike',\n",
       " 'research',\n",
       " 'Decian',\n",
       " 'frozen',\n",
       " 'seal',\n",
       " 'contract',\n",
       " 'unhappiness',\n",
       " '}',\n",
       " 'cast',\n",
       " 'win',\n",
       " 'perfectly',\n",
       " 'fear',\n",
       " '264',\n",
       " 'next',\n",
       " '.a',\n",
       " 'ply',\n",
       " 'Sedan',\n",
       " 'wait',\n",
       " 'hot',\n",
       " 'everybody',\n",
       " 'yurt',\n",
       " 'lightly',\n",
       " 'mess',\n",
       " 'Uncompahgre',\n",
       " 'final',\n",
       " 'rejecter',\n",
       " 'Bud',\n",
       " 'Clark',\n",
       " 'vehicle',\n",
       " 'attach',\n",
       " 'Kids',\n",
       " 'without',\n",
       " 'entitle',\n",
       " 'becap',\n",
       " 'hoop',\n",
       " 'prefer',\n",
       " 'Regga',\n",
       " 'bedroom',\n",
       " 'loud',\n",
       " 'summoner',\n",
       " 'Aptera',\n",
       " 'carse',\n",
       " 'session',\n",
       " 'Phlomis',\n",
       " '908sq',\n",
       " 'Kurd',\n",
       " 'map',\n",
       " 'footpath',\n",
       " 'reactor',\n",
       " 'team',\n",
       " 'Dada',\n",
       " 'Magellan',\n",
       " 'talk',\n",
       " 'purchase',\n",
       " 'cow',\n",
       " 'tete',\n",
       " 'reading',\n",
       " 'Dehaites',\n",
       " 'surround',\n",
       " 'widen',\n",
       " 'leer',\n",
       " 'Savara',\n",
       " 'frustration',\n",
       " 'Anacharis',\n",
       " 'Gujrati',\n",
       " 'shide',\n",
       " 'weaken',\n",
       " 'nice',\n",
       " 'downhill',\n",
       " 'meddle',\n",
       " 'soldier',\n",
       " 'Sakalava',\n",
       " 'Votish',\n",
       " 'randing',\n",
       " 'lighting',\n",
       " 'common',\n",
       " 'building',\n",
       " 'Thoroughbred',\n",
       " 'Meredithian',\n",
       " 'Java',\n",
       " 'Arean',\n",
       " 'neutral',\n",
       " 'unite',\n",
       " 'patcher',\n",
       " 'taglike',\n",
       " 'shore',\n",
       " 'die',\n",
       " 'shadow',\n",
       " 'middleman',\n",
       " 'peacefully',\n",
       " 'draftee',\n",
       " 'favorite',\n",
       " 'edit',\n",
       " 'news',\n",
       " 'Telegn',\n",
       " 'Strad',\n",
       " 'Lallan',\n",
       " 'nach',\n",
       " 'tumble',\n",
       " 'Stalinism',\n",
       " 'Atta',\n",
       " 'salon',\n",
       " 'Whitechapel',\n",
       " 'scratch',\n",
       " '......',\n",
       " 'seminar',\n",
       " ']',\n",
       " 'raincoat',\n",
       " 'Taruma',\n",
       " 'Lif',\n",
       " 'yesterday',\n",
       " 'dare',\n",
       " 'Amita',\n",
       " 'hawk',\n",
       " 'Gorkhali',\n",
       " 'industrial',\n",
       " 'vee',\n",
       " 'graft',\n",
       " 'condemn',\n",
       " 'Freesia',\n",
       " 'dual',\n",
       " 'Waterberg',\n",
       " 'Vivek',\n",
       " 'Kaf',\n",
       " 'inbond',\n",
       " 'Maliki',\n",
       " 'Cat',\n",
       " 'plaint',\n",
       " 'Neri',\n",
       " \"'CONDITION\",\n",
       " 'Fin',\n",
       " 'fine',\n",
       " 'change',\n",
       " 'take',\n",
       " 'callus',\n",
       " 'le',\n",
       " 'criminal',\n",
       " 'ajaja',\n",
       " 'fan',\n",
       " 'hand',\n",
       " '30+',\n",
       " 'fund',\n",
       " 'swording',\n",
       " 'Bare',\n",
       " 'Unrussian',\n",
       " 'Londony',\n",
       " 'rescue',\n",
       " 'Puritan',\n",
       " 'Abo',\n",
       " 'Psidium',\n",
       " 'almon',\n",
       " 'syndrome',\n",
       " 'clue',\n",
       " 'huge',\n",
       " 'publisher',\n",
       " 'European',\n",
       " 'hold',\n",
       " 'access',\n",
       " '.30',\n",
       " 'robe',\n",
       " 'Frieda',\n",
       " 'fight',\n",
       " 'wi',\n",
       " 'opah',\n",
       " 'occasion',\n",
       " '3',\n",
       " 'ruckus',\n",
       " 'controversy',\n",
       " 'papmeat',\n",
       " 'chord',\n",
       " 'thing',\n",
       " 'train',\n",
       " 'keynote',\n",
       " 'lifetime',\n",
       " 'vow',\n",
       " 'Boothian',\n",
       " 'venue',\n",
       " 'salad',\n",
       " 'nail',\n",
       " 'depict',\n",
       " 'Helot',\n",
       " 'Boney',\n",
       " '31',\n",
       " 'needer',\n",
       " 'recipe',\n",
       " 'origin',\n",
       " 'March',\n",
       " 'theory',\n",
       " 'Zabaean',\n",
       " 'fog',\n",
       " 'fleet',\n",
       " 'markup',\n",
       " 'debby',\n",
       " 'Jaina',\n",
       " 'development',\n",
       " 'Garrya',\n",
       " '.11',\n",
       " 'inspiring',\n",
       " \"'slaps\",\n",
       " 'No',\n",
       " 'Aira',\n",
       " 'impartial',\n",
       " 'dress',\n",
       " 'hing',\n",
       " 'politics',\n",
       " 'begani',\n",
       " 'Senci',\n",
       " 'Rashti',\n",
       " 'Mura',\n",
       " 'gat',\n",
       " 'Downing',\n",
       " 'Maurice',\n",
       " 'story',\n",
       " '.32',\n",
       " 'early',\n",
       " 'paramilitary',\n",
       " 'cain',\n",
       " 'Bhumij',\n",
       " 'kmet',\n",
       " ':30',\n",
       " 'wanter',\n",
       " 'enter',\n",
       " 'grow',\n",
       " 'lower',\n",
       " 'boltmaker',\n",
       " 'Sinto',\n",
       " 'integral',\n",
       " 'Sam',\n",
       " 'popple',\n",
       " 'law',\n",
       " 'jina',\n",
       " 'healthy',\n",
       " 'bronchitis',\n",
       " 'performance',\n",
       " 'homework',\n",
       " 'far',\n",
       " 'Starbuck',\n",
       " 'mind',\n",
       " 'battle',\n",
       " 'kop',\n",
       " 'functionary',\n",
       " 'respect',\n",
       " 'Kyle',\n",
       " 'got',\n",
       " 'Gerasene',\n",
       " 'cuddle',\n",
       " 'Hon',\n",
       " 'Predentata',\n",
       " 'telecode',\n",
       " 'impeachment',\n",
       " 'nose',\n",
       " 'Larnaudian',\n",
       " 'Branchiosaurus',\n",
       " 'Italianation',\n",
       " '.4',\n",
       " 'puppet',\n",
       " 'festivity',\n",
       " 'notification',\n",
       " 'bad',\n",
       " 'gif',\n",
       " 'evacuate',\n",
       " '17',\n",
       " 'understand',\n",
       " 'substantially',\n",
       " '.Thanks',\n",
       " 'Andy',\n",
       " 'trouble',\n",
       " 'introit',\n",
       " 'Venkata',\n",
       " 'Brighteyes',\n",
       " 'Haraya',\n",
       " 'Serrano',\n",
       " 'gift',\n",
       " 'constantly',\n",
       " 'embarrassing',\n",
       " 'emotional',\n",
       " 'Mariana',\n",
       " 'choice',\n",
       " 'banga',\n",
       " 'Sinic',\n",
       " 'newel',\n",
       " 'Fingall',\n",
       " 'bubby',\n",
       " 'avenger',\n",
       " 'daystreak',\n",
       " 'stretcher',\n",
       " 'welfare',\n",
       " '938',\n",
       " 'shall',\n",
       " 'attractive',\n",
       " 'conduce',\n",
       " 'acid',\n",
       " 'Derek',\n",
       " 'Theresa',\n",
       " 'tomorrow',\n",
       " 'calendar',\n",
       " 'afraid',\n",
       " 'posse',\n",
       " 'recovery',\n",
       " 'farrow',\n",
       " 'flacked',\n",
       " 'waned',\n",
       " '24-20',\n",
       " 'Shankar',\n",
       " 'pitch',\n",
       " 'choke',\n",
       " 'Ruskinian',\n",
       " 'retard',\n",
       " 'Aristides',\n",
       " 'guy',\n",
       " 'Chris',\n",
       " 'spectrum',\n",
       " 'astound',\n",
       " 'cancel',\n",
       " 'away',\n",
       " 'shaikh',\n",
       " 'turner',\n",
       " 'must',\n",
       " 'Nama',\n",
       " 'savanilla',\n",
       " 'author',\n",
       " 'embrail',\n",
       " 'Damia',\n",
       " 'privacy',\n",
       " 'Jem',\n",
       " 'wreathmaking',\n",
       " 'dye',\n",
       " 'ingredient',\n",
       " 'confuse',\n",
       " 'murderer',\n",
       " 'translator',\n",
       " 'bomb',\n",
       " 'auction',\n",
       " 'climax',\n",
       " 'watering',\n",
       " 'Woody',\n",
       " 'Acestes',\n",
       " 'realize',\n",
       " 'improve',\n",
       " 'anyways',\n",
       " 'Boyd',\n",
       " 'eaten',\n",
       " 'visa',\n",
       " 'jadish',\n",
       " 'tune',\n",
       " 'suddenly',\n",
       " 'bibb',\n",
       " 'tamperer',\n",
       " 'disable',\n",
       " 'lab',\n",
       " 'Ifugao',\n",
       " 'capture',\n",
       " 'sleep',\n",
       " 'Scaticook',\n",
       " 'Gammarus',\n",
       " 'imagination',\n",
       " 'true',\n",
       " 'Baal',\n",
       " 'tackle',\n",
       " 'disruptive',\n",
       " 'Felis',\n",
       " 'bluebell',\n",
       " 'ear',\n",
       " 'January',\n",
       " 'marry',\n",
       " 'Antenor',\n",
       " 'twitter',\n",
       " 'Xenopteri',\n",
       " 'smiling',\n",
       " 'Bos',\n",
       " 'Nadeem',\n",
       " 'tangun',\n",
       " 'religious',\n",
       " 'broken',\n",
       " 'Chac',\n",
       " 'redweed',\n",
       " 'operation',\n",
       " 'structure',\n",
       " 'yes',\n",
       " 'Feme',\n",
       " 'legislative',\n",
       " 'agree',\n",
       " 'Rebecca',\n",
       " 'Swede',\n",
       " 'Frances',\n",
       " 'tourist',\n",
       " 'Monel',\n",
       " 'leaker',\n",
       " 'assaulter',\n",
       " 'invite',\n",
       " 'melt',\n",
       " 'Chechen',\n",
       " 'Miao',\n",
       " 'fly',\n",
       " 'Serinus',\n",
       " 'Nannette',\n",
       " 'hopefully',\n",
       " 'Ofo',\n",
       " '32-',\n",
       " 'Mucker',\n",
       " 'X',\n",
       " '7:30',\n",
       " 'Exchangite',\n",
       " 'Devon',\n",
       " 'business',\n",
       " 'job',\n",
       " 'Utahan',\n",
       " 'stupid',\n",
       " 'Bat',\n",
       " 'overwork',\n",
       " 'Basella',\n",
       " 'Wend',\n",
       " 'employee',\n",
       " '%',\n",
       " 'Finn',\n",
       " 'survive',\n",
       " 'economic',\n",
       " 'knowledge',\n",
       " 'pig',\n",
       " 'tap',\n",
       " 'Liza',\n",
       " 'career',\n",
       " 'heterothallic',\n",
       " 'accomplish',\n",
       " 'resist',\n",
       " 'acca',\n",
       " 'envoy',\n",
       " \"'nationalist\",\n",
       " 'ammunition',\n",
       " 'Angie',\n",
       " 'nimmer',\n",
       " 'turf',\n",
       " 'reason',\n",
       " 'semester',\n",
       " 'Rohilla',\n",
       " 'patience',\n",
       " 'sport',\n",
       " 'fill',\n",
       " 'adjournment',\n",
       " 'assure',\n",
       " 'cultist',\n",
       " 'R',\n",
       " 'party',\n",
       " 'high',\n",
       " 'gabi',\n",
       " '.5',\n",
       " 'code',\n",
       " 'son',\n",
       " 'philosophy',\n",
       " 'Bourbon',\n",
       " 'somehow',\n",
       " 'Gaia',\n",
       " 'link',\n",
       " 'Perca',\n",
       " 'affair',\n",
       " 'ease',\n",
       " 'Flaminian',\n",
       " 'Stefan',\n",
       " 'upstairs',\n",
       " 'buck',\n",
       " 'Lead',\n",
       " 'Crambidae',\n",
       " 'Clay',\n",
       " 'system',\n",
       " 'revive',\n",
       " 'Dargo',\n",
       " 'Poll',\n",
       " 'towards',\n",
       " 'Mya',\n",
       " 'bullet',\n",
       " 'war',\n",
       " 'homage',\n",
       " 'soon',\n",
       " 'Rhopalura',\n",
       " 'backstage',\n",
       " 'Pontederia',\n",
       " 'cosubordinate',\n",
       " 'inspiration',\n",
       " 'moment',\n",
       " 'pussy',\n",
       " 'outside',\n",
       " 'worse',\n",
       " 'Miami',\n",
       " 'wow',\n",
       " 'ssu',\n",
       " 'protect',\n",
       " 'infilling',\n",
       " 'egg',\n",
       " 'refer',\n",
       " '2016',\n",
       " 'Blackhander',\n",
       " \"'selfie\",\n",
       " 'Saturday',\n",
       " 'violation',\n",
       " 'explainer',\n",
       " 'Beid',\n",
       " 'bean',\n",
       " 'coalition',\n",
       " 'etch',\n",
       " 'skull',\n",
       " 'staff',\n",
       " 'Zeus',\n",
       " 'abundance',\n",
       " 'Manny',\n",
       " 'Philip',\n",
       " 'founder',\n",
       " 'syne',\n",
       " 'note',\n",
       " 'jeel',\n",
       " 'nonsecret',\n",
       " 'ride',\n",
       " 'Kanwar',\n",
       " '6.4',\n",
       " 'tech',\n",
       " 'cover',\n",
       " 'Tod',\n",
       " 'comboy',\n",
       " 'christener',\n",
       " 'sound',\n",
       " 'easier',\n",
       " 'engage',\n",
       " 'Ree',\n",
       " 'call',\n",
       " 'velvet',\n",
       " '4:30',\n",
       " 'sigh',\n",
       " 'Leander',\n",
       " 'hang',\n",
       " 'anoa',\n",
       " '/Rosi',\n",
       " '?',\n",
       " 'eu',\n",
       " 'muffle',\n",
       " 'gang',\n",
       " 'nikau',\n",
       " 'manna',\n",
       " 'suitable',\n",
       " 'Sarcoptes',\n",
       " 'Halleyan',\n",
       " 'destination',\n",
       " 'tender',\n",
       " 'Hima',\n",
       " 'crowder',\n",
       " 'extra',\n",
       " 'holiday',\n",
       " 'Ban',\n",
       " 'musical',\n",
       " 'tabla',\n",
       " 'Flamingant',\n",
       " 'ill',\n",
       " '.33',\n",
       " 'attender',\n",
       " 'hogwort',\n",
       " '11am',\n",
       " 'Passionist',\n",
       " 'explain',\n",
       " 'Dob',\n",
       " 'motivation',\n",
       " 'Lokindra',\n",
       " '2NE1',\n",
       " 'achieve',\n",
       " 'cub',\n",
       " 'earliness',\n",
       " 'appointe',\n",
       " 'exporter',\n",
       " 'detail',\n",
       " 'imposition',\n",
       " 'protocol',\n",
       " 'xanthamic',\n",
       " 'dynastical',\n",
       " '1.2',\n",
       " 'Messalian',\n",
       " '6.75',\n",
       " 'uplifting',\n",
       " 'know',\n",
       " 'night',\n",
       " 'corruption',\n",
       " 'man',\n",
       " 'missel',\n",
       " 'receive',\n",
       " 'Sri',\n",
       " 'lend',\n",
       " 'handout',\n",
       " 'confident',\n",
       " 'lamp',\n",
       " 'mutiny',\n",
       " '1863',\n",
       " 'Dan',\n",
       " 'wish',\n",
       " 'Kieran',\n",
       " 'Phil',\n",
       " 'earl',\n",
       " 'onside',\n",
       " 'wipe',\n",
       " 'ballot',\n",
       " 'stress',\n",
       " 'also',\n",
       " '.22',\n",
       " 'strip',\n",
       " 'Ann',\n",
       " 'emotion',\n",
       " 'damage',\n",
       " 'comparison',\n",
       " 'equip',\n",
       " '28',\n",
       " 'M',\n",
       " 'Hehe',\n",
       " 'Ming',\n",
       " 'attitude',\n",
       " 'i',\n",
       " 'duddies',\n",
       " 'cad',\n",
       " '40000',\n",
       " '42nd',\n",
       " 'suffer',\n",
       " 'dump',\n",
       " '401',\n",
       " 'chitin',\n",
       " 'wet',\n",
       " 'proclaimer',\n",
       " 'nix',\n",
       " 'extended',\n",
       " 'hear',\n",
       " 'fret',\n",
       " 'Winston',\n",
       " 'antique',\n",
       " 'vanish',\n",
       " 'epic',\n",
       " 'Clerus',\n",
       " 'trek',\n",
       " 'batch',\n",
       " 'formidable',\n",
       " 'close',\n",
       " 'head',\n",
       " 'estimate',\n",
       " 'boycott',\n",
       " 'critique',\n",
       " 'tactful',\n",
       " 'masticate',\n",
       " 'Bitis',\n",
       " 'Strumella',\n",
       " 'Prakash',\n",
       " 'thig',\n",
       " '4/The',\n",
       " 'act',\n",
       " 'arm',\n",
       " 'vision',\n",
       " 'Assamese',\n",
       " 'exclude',\n",
       " 'lift',\n",
       " 'Wolf',\n",
       " 'Bim',\n",
       " '07:00AM',\n",
       " 'Cain',\n",
       " 'Ignorantine',\n",
       " 'Maranha',\n",
       " 'midnight',\n",
       " 'pan',\n",
       " 'Grant',\n",
       " 'bat',\n",
       " 'visit',\n",
       " 'regard',\n",
       " '14',\n",
       " 'Sanand',\n",
       " 'Kanarese',\n",
       " 'Washington',\n",
       " 'Beck',\n",
       " 'rouse',\n",
       " 'effective',\n",
       " '200000',\n",
       " 'kiddy',\n",
       " 'pas',\n",
       " 'depth',\n",
       " 'redesign',\n",
       " 'death',\n",
       " 'big',\n",
       " 'behaviour',\n",
       " 'Puno',\n",
       " 'lover',\n",
       " 'muga',\n",
       " 'overnight',\n",
       " 'Friday',\n",
       " 'Thuan',\n",
       " 'Asiatically',\n",
       " 'fact',\n",
       " 'bore',\n",
       " 'celebrate',\n",
       " 'poppin',\n",
       " 'seedling',\n",
       " 'Oneida',\n",
       " 'Teri',\n",
       " 'Lamba',\n",
       " \"'control\",\n",
       " 'Diana',\n",
       " 'Del',\n",
       " '^',\n",
       " 'servant',\n",
       " 'thanks',\n",
       " 'tanker',\n",
       " 'Russian',\n",
       " 'fell',\n",
       " 'punishment',\n",
       " 'Wallon',\n",
       " 'concern',\n",
       " 'wear',\n",
       " 'initially',\n",
       " 'solely',\n",
       " 'edgy',\n",
       " 'crave',\n",
       " 'minute',\n",
       " '51',\n",
       " 'dead',\n",
       " 'Laurel',\n",
       " 'pa',\n",
       " 'brighten',\n",
       " 'repeal',\n",
       " 'tell',\n",
       " 'challengingly',\n",
       " 'nabber',\n",
       " 'Tony',\n",
       " 'sluggish',\n",
       " 'destroyer',\n",
       " 'taste',\n",
       " 'Emys',\n",
       " 'home',\n",
       " 'rusty',\n",
       " '500m',\n",
       " 'merit',\n",
       " '86',\n",
       " 'much',\n",
       " 'Acalypterae',\n",
       " 'Nagari',\n",
       " 'heroic',\n",
       " 'practice',\n",
       " 'engross',\n",
       " 'jig',\n",
       " 'Meg',\n",
       " 'Vertebrata',\n",
       " 'nonetheless',\n",
       " 'unqualified',\n",
       " 'political',\n",
       " 'Askr',\n",
       " 'oloroso',\n",
       " 'assist',\n",
       " 'host',\n",
       " '4.1',\n",
       " 'Ray',\n",
       " 'hotter',\n",
       " 'personnel',\n",
       " 'Tartan',\n",
       " 'Steve',\n",
       " 'seventh',\n",
       " 'stand',\n",
       " 'eclipse',\n",
       " 'oldish',\n",
       " 'precious',\n",
       " 'restrictive',\n",
       " 'appropre',\n",
       " 'Khila',\n",
       " 'writer',\n",
       " 'hubby',\n",
       " 'crocodile',\n",
       " \"'utmost\",\n",
       " 'sadly',\n",
       " 'Withania',\n",
       " 'develop',\n",
       " 'expensive',\n",
       " 'rappe',\n",
       " 'Bedford',\n",
       " \"'architecture\",\n",
       " 'trooper',\n",
       " 'Listerine',\n",
       " 'update',\n",
       " 'Case',\n",
       " 'Hank',\n",
       " 'obvious',\n",
       " 'yeo',\n",
       " 'mock',\n",
       " 'subsidist',\n",
       " 'Moran',\n",
       " 'bureaucrat',\n",
       " 'anywhere',\n",
       " 'ca',\n",
       " 'easy',\n",
       " 'Nomeidae',\n",
       " 'Berteroa',\n",
       " 'bottle',\n",
       " 'spat',\n",
       " 'polarization',\n",
       " 'Nacionalista',\n",
       " \"'frank\",\n",
       " 'veer',\n",
       " 'suicide',\n",
       " 'thrive',\n",
       " 'Moore',\n",
       " 'sort',\n",
       " '1-party',\n",
       " '7',\n",
       " 'pavement',\n",
       " 'askingly',\n",
       " 'Steen',\n",
       " 'paint',\n",
       " 'Sudder',\n",
       " 'Noachite',\n",
       " 'Part',\n",
       " 'cheer',\n",
       " 'resilience',\n",
       " 'Hugelia',\n",
       " 'art',\n",
       " 'park',\n",
       " 'social',\n",
       " 'mighty',\n",
       " 'reality',\n",
       " 'spot',\n",
       " 'beautifully',\n",
       " 'Holly',\n",
       " 'Heliaea',\n",
       " 'fee',\n",
       " 'factory',\n",
       " 'Ugric',\n",
       " 'rupee',\n",
       " 'dethrone',\n",
       " 'somebody',\n",
       " 'tendence',\n",
       " 'go',\n",
       " '4.32',\n",
       " 'Tor',\n",
       " 'shoot',\n",
       " 'Amerind',\n",
       " 'decode',\n",
       " 'flooding',\n",
       " 'Bab',\n",
       " 'bigha',\n",
       " 'Sorrel',\n",
       " 'stick',\n",
       " 'Mauri',\n",
       " 'removal',\n",
       " 'Tilletia',\n",
       " 'urge',\n",
       " 'teacher',\n",
       " 'matsu',\n",
       " 'sick',\n",
       " 'jed',\n",
       " 'step',\n",
       " 'express',\n",
       " '4-member',\n",
       " 'Miguel',\n",
       " 'Memnon',\n",
       " 'trust',\n",
       " 'sull',\n",
       " 'communalism',\n",
       " 'amid',\n",
       " 'Shaker',\n",
       " 'nonmetal',\n",
       " 'Storting',\n",
       " 'Bullom',\n",
       " 'Triturus',\n",
       " 'Spencer',\n",
       " 'agency',\n",
       " 'see',\n",
       " 'guilty',\n",
       " 'jean',\n",
       " 'shear',\n",
       " 'point',\n",
       " 'discredit',\n",
       " 'otic',\n",
       " 'senior',\n",
       " 'Cinemascope',\n",
       " 'risk',\n",
       " 'Easter',\n",
       " 'shun',\n",
       " 'lender',\n",
       " 'bottom',\n",
       " 'courage',\n",
       " '16000',\n",
       " 'prevent',\n",
       " 'carina',\n",
       " 'Studite',\n",
       " 'Yahuna',\n",
       " 'correct',\n",
       " 'Astragalus',\n",
       " 'goal',\n",
       " 'decad',\n",
       " 'left',\n",
       " 'linear',\n",
       " 'sentiment',\n",
       " 'Cole',\n",
       " 'Sanjib',\n",
       " 'ware',\n",
       " 'mapo',\n",
       " 'range',\n",
       " 'swanky',\n",
       " 'Teague',\n",
       " 'trash',\n",
       " 'Satan',\n",
       " 'Chalina',\n",
       " 'certificate',\n",
       " 'complainingly',\n",
       " 'import',\n",
       " 'industry',\n",
       " 'constitution',\n",
       " 'slap',\n",
       " 'rename',\n",
       " '20',\n",
       " 'stop',\n",
       " 'escalate',\n",
       " 'turn',\n",
       " 'Wagnerite',\n",
       " 'honey',\n",
       " 'compensate',\n",
       " 'studdie',\n",
       " 'sleepy',\n",
       " 'accent',\n",
       " 'guest',\n",
       " 'feature',\n",
       " 'seat',\n",
       " 'wont',\n",
       " 'secundate',\n",
       " '.39',\n",
       " 'catch',\n",
       " 'didna',\n",
       " 'property',\n",
       " 'brutal',\n",
       " \"'A7a\",\n",
       " 'involve',\n",
       " 'Maine',\n",
       " 'jacket',\n",
       " 'loll',\n",
       " 'Selena',\n",
       " 'effect',\n",
       " 'tired',\n",
       " 'appropriate',\n",
       " 'gain',\n",
       " 'dong',\n",
       " 'Eirene',\n",
       " 'helder',\n",
       " 'Saidi',\n",
       " 'honest',\n",
       " 'genre',\n",
       " 'contribution',\n",
       " 'Media',\n",
       " 'pitfall',\n",
       " 'Medish',\n",
       " 'handwork',\n",
       " 'Umaua',\n",
       " 'Sundar',\n",
       " 'chaotic',\n",
       " 'colour',\n",
       " 'completely',\n",
       " 'basketball',\n",
       " 'adorable',\n",
       " 'Damon',\n",
       " 'dhan',\n",
       " 'bench',\n",
       " 'killing',\n",
       " '4',\n",
       " 'I',\n",
       " 'Greenland',\n",
       " 'Ansarie',\n",
       " 'deviation',\n",
       " 'someone',\n",
       " 'professional',\n",
       " 'Heracliteanism',\n",
       " 'wheat',\n",
       " 'taking',\n",
       " 'triangulation',\n",
       " 'underway',\n",
       " 'Danization',\n",
       " 'crescent',\n",
       " 'Price',\n",
       " 'service',\n",
       " 'Bihari',\n",
       " 'female',\n",
       " 'cold',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unic_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Узнаем полученную точность модели<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/val/.local/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "word_exist_accuracy_score = model_selection_word_exist(lem_misspel_df, unic_words)\n",
    "word_count_accuracy_score = model_selection_word_count(lem_misspel_df, unic_words)\n",
    "tfidf_accuracy_score = model_selection_tfidf(lem_misspel_df, unic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score by word exist: 0.5133214920071048\n",
      "Accuracy score by word count: 0.933392539964476\n",
      "Fccuracy score by tfidf: 0.522202486678508\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Accuracy score by word exist: {word_exist_accuracy_score}\n",
    "Accuracy score by word count: {word_count_accuracy_score}\n",
    "Fccuracy score by tfidf: {tfidf_accuracy_score}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
