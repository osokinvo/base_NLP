{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Стемминг<h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Стемминг – это своего рода нормализация слов. Нормализация – это метод, при котором набор слов в предложении преобразуется в последовательность, чтобы сократить время поиска. Слова, которые имеют то же значение, но имеют некоторые различия в зависимости от контекста или предложения, нормализуются. Другими словами, есть одно корневое слово, но есть много вариантов одних и тех же слов. Например, корневое слово «есть» и его вариации «есть, есть, есть и так далее». Точно так же, с помощью Stemming, мы можем найти корневое слово любых вариаций.<h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Импортируем библиотеки<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/val/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/val/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "from model_selction import model_selection_word_count, model_selection_word_exist, model_selection_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mp00_tweets.zip\u001b[0m*         \u001b[01;32mprocessedNeutral.csv\u001b[0m*\n",
      "\u001b[01;32mprocessedNegative.csv\u001b[0m*  \u001b[01;32mprocessedPositive.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>В качестве примера рассмотрим содержимое файла 'processedNegative.csv' после применения метода<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'unhappi',\n",
       " 'dog',\n",
       " 'like',\n",
       " 'though',\n",
       " 'talk',\n",
       " 'driver',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'goingh',\n",
       " 'said',\n",
       " \"'d\",\n",
       " 'love',\n",
       " 'go',\n",
       " 'new',\n",
       " 'york',\n",
       " 'sinc',\n",
       " 'trump',\n",
       " \"'s\",\n",
       " 'probabl',\n",
       " 'doe',\n",
       " 'anybodi',\n",
       " 'know',\n",
       " 'rand',\n",
       " \"'s\",\n",
       " 'like',\n",
       " 'fall',\n",
       " 'dollar',\n",
       " '?',\n",
       " 'i',\n",
       " 'got',\n",
       " 'money',\n",
       " 'i',\n",
       " 'need',\n",
       " 'chang',\n",
       " 'r',\n",
       " 'keep',\n",
       " 'get',\n",
       " 'stronger',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'go',\n",
       " 'gig',\n",
       " 'liverpool',\n",
       " 'unhappi',\n",
       " 'there',\n",
       " 'isnt',\n",
       " 'new',\n",
       " 'riverdal',\n",
       " 'tonight',\n",
       " '?',\n",
       " 'unhappi',\n",
       " \"'s\",\n",
       " 'a',\n",
       " '*',\n",
       " 'dy',\n",
       " 'guy',\n",
       " 'pop',\n",
       " 'asia',\n",
       " 'translat',\n",
       " \"'ll\",\n",
       " 'prob',\n",
       " 'go',\n",
       " 'around',\n",
       " 'au',\n",
       " 'unhappi',\n",
       " 'who',\n",
       " \"'s\",\n",
       " 'chair',\n",
       " \"'re\",\n",
       " 'sit',\n",
       " '?',\n",
       " 'is',\n",
       " 'i',\n",
       " 'find',\n",
       " '.',\n",
       " 'everyon',\n",
       " 'know',\n",
       " '.',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'shame',\n",
       " 'pu',\n",
       " \"n't\",\n",
       " 'like',\n",
       " 'jitteri',\n",
       " 'caffein',\n",
       " 'make',\n",
       " 'sad',\n",
       " 'my',\n",
       " 'area',\n",
       " \"'s\",\n",
       " 'list',\n",
       " 'unhappi',\n",
       " 'think',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'go',\n",
       " 'libdem',\n",
       " 'anyway',\n",
       " 'i',\n",
       " 'want',\n",
       " 'fun',\n",
       " 'plan',\n",
       " 'weekend',\n",
       " 'unhappi',\n",
       " 'when',\n",
       " 'notic',\n",
       " '.',\n",
       " 'unhappi',\n",
       " '?',\n",
       " 'ahhhhh',\n",
       " '!',\n",
       " 'you',\n",
       " 'recogn',\n",
       " 'logan',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'cinemax',\n",
       " 'show',\n",
       " 'bad',\n",
       " 'track',\n",
       " 'record',\n",
       " 'get',\n",
       " 'cancel',\n",
       " 'unhappi',\n",
       " 'errr',\n",
       " 'dude',\n",
       " '....',\n",
       " 'they',\n",
       " \"'re\",\n",
       " 'gone',\n",
       " 'unhappi',\n",
       " 'ask',\n",
       " 'leagu',\n",
       " 'memeb',\n",
       " 'check',\n",
       " 'guy',\n",
       " 'go',\n",
       " 'not',\n",
       " 'sad',\n",
       " 'whi',\n",
       " 'would',\n",
       " 'harvey',\n",
       " 'go',\n",
       " 'prison',\n",
       " '?',\n",
       " 'unhappi',\n",
       " 'miss',\n",
       " 'cri',\n",
       " 'seasid',\n",
       " 'area',\n",
       " '.',\n",
       " 'becoz',\n",
       " 'depend',\n",
       " 'promot',\n",
       " 'wast',\n",
       " 'hardwork',\n",
       " 'team',\n",
       " 'i',\n",
       " 'thought',\n",
       " \"'ll\",\n",
       " 'save',\n",
       " 'cri',\n",
       " 'major',\n",
       " 'waffl',\n",
       " 'crave',\n",
       " 'right',\n",
       " 'sad',\n",
       " 'cant',\n",
       " 'speak',\n",
       " 'japanes',\n",
       " ':',\n",
       " ':',\n",
       " '(',\n",
       " 'peopl',\n",
       " 'stuff',\n",
       " 'like',\n",
       " 'unhappi',\n",
       " 'pleas',\n",
       " 'stop',\n",
       " 'confin',\n",
       " 'anim',\n",
       " 'zoo',\n",
       " 'unhappi',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'shoyould',\n",
       " 'tell',\n",
       " 'get',\n",
       " 'fuck',\n",
       " 'social',\n",
       " 'media',\n",
       " 'byout',\n",
       " 'also',\n",
       " 'feel',\n",
       " 'realli',\n",
       " 'mean',\n",
       " 'unhappi',\n",
       " 'silenc',\n",
       " 'love',\n",
       " 'yoyou',\n",
       " 'hope',\n",
       " 'yoyour',\n",
       " 'okay',\n",
       " 'miss',\n",
       " 'huhu',\n",
       " 'busi',\n",
       " 'unhappi',\n",
       " 'extend',\n",
       " 'famili',\n",
       " '.',\n",
       " '12',\n",
       " 'ppl.ahh',\n",
       " 'want',\n",
       " 'show',\n",
       " 'oh',\n",
       " 'my',\n",
       " 'girl',\n",
       " 'dorki',\n",
       " 'play',\n",
       " 'game',\n",
       " 'got',\n",
       " 'delet',\n",
       " 'unhappi',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'unhappi',\n",
       " 'jami',\n",
       " 'pleas',\n",
       " 'reset',\n",
       " 'cga',\n",
       " 'grandfin',\n",
       " 'server',\n",
       " '...',\n",
       " 'administr',\n",
       " 'respond',\n",
       " 'unhappi',\n",
       " 'noooooooo',\n",
       " 'you',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'miss',\n",
       " 'the',\n",
       " 'buffet',\n",
       " 'unhappi',\n",
       " 'take',\n",
       " 'care',\n",
       " 'ain',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'i',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'vote',\n",
       " 'unhappi',\n",
       " 'instant',\n",
       " 'messag',\n",
       " 'jealou',\n",
       " 'okay',\n",
       " 'unhappi',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'haha',\n",
       " 'bruno',\n",
       " 'wait',\n",
       " 'final',\n",
       " 'first',\n",
       " \"'m\",\n",
       " 'enlist',\n",
       " 'pleas',\n",
       " 'turn',\n",
       " 'like',\n",
       " 'unhappi',\n",
       " 'unhappi',\n",
       " 'come',\n",
       " 'peopl',\n",
       " 'like',\n",
       " 'children',\n",
       " \"'s\",\n",
       " 'state',\n",
       " 'intervent',\n",
       " 'ouchhhhh',\n",
       " 'unhappi',\n",
       " 'help',\n",
       " '...',\n",
       " 'i',\n",
       " 'want',\n",
       " 'stop',\n",
       " 'tweet',\n",
       " '.',\n",
       " 'all',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'endless',\n",
       " 'suffer',\n",
       " 'pain',\n",
       " '.',\n",
       " 'i',\n",
       " 'tri',\n",
       " 'deactiv',\n",
       " 'mani',\n",
       " 'time',\n",
       " '...',\n",
       " 'save',\n",
       " '...',\n",
       " 'unhappi',\n",
       " 'for',\n",
       " 'ask',\n",
       " 'applic',\n",
       " 'kana',\n",
       " 'kanji',\n",
       " 'funtim',\n",
       " '!',\n",
       " 'sadli',\n",
       " 'look',\n",
       " 'like',\n",
       " 'io',\n",
       " '11',\n",
       " 'due',\n",
       " 'kill',\n",
       " 'unhappi',\n",
       " 'yeah',\n",
       " 'updat',\n",
       " '16.04',\n",
       " 'froze',\n",
       " 'time',\n",
       " '.',\n",
       " 'then',\n",
       " 'went',\n",
       " '16.10',\n",
       " 'froze',\n",
       " 'mid',\n",
       " 'instal',\n",
       " '.',\n",
       " 'wait',\n",
       " '3hr',\n",
       " \"'d\",\n",
       " 'pull',\n",
       " 'plug',\n",
       " 'cri',\n",
       " 'shaandaar',\n",
       " 'zabardast',\n",
       " 'anoth',\n",
       " 'atbb',\n",
       " \"'s\",\n",
       " 'way',\n",
       " '!',\n",
       " 'i',\n",
       " 'wish',\n",
       " 'srk',\n",
       " 'sir',\n",
       " 'start',\n",
       " 'sign',\n",
       " 'good',\n",
       " 'movi',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " 'want',\n",
       " 'jabe',\n",
       " 'cri',\n",
       " 'sociopath',\n",
       " 'full',\n",
       " 'raid',\n",
       " 'gear',\n",
       " 'sad',\n",
       " 'when',\n",
       " 'say',\n",
       " 'hi',\n",
       " 'sunshin',\n",
       " '?',\n",
       " 'unhappi',\n",
       " 'feel',\n",
       " 'bad',\n",
       " 'ahaha',\n",
       " 'unhappi',\n",
       " \"'s\",\n",
       " 'get',\n",
       " 'harder',\n",
       " 'harder',\n",
       " 'stay',\n",
       " 'unhappi',\n",
       " 'hi',\n",
       " 'face',\n",
       " 'look',\n",
       " 'bloat',\n",
       " 'unhappi',\n",
       " 'babi',\n",
       " 'get',\n",
       " 'well',\n",
       " 'soon',\n",
       " 'fuck',\n",
       " '.',\n",
       " 'tri',\n",
       " 'chang',\n",
       " 'set',\n",
       " 'still',\n",
       " 'india',\n",
       " '.',\n",
       " 'unhappi',\n",
       " 'talk',\n",
       " 'driver',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'goingh',\n",
       " 'said',\n",
       " \"'d\",\n",
       " 'love',\n",
       " 'go',\n",
       " 'new',\n",
       " 'york',\n",
       " 'sinc',\n",
       " 'trump',\n",
       " \"'s\",\n",
       " 'probabl',\n",
       " 'not.1',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'forget',\n",
       " \"'s\",\n",
       " 'also',\n",
       " 'gabriel',\n",
       " 'tenma',\n",
       " 'white',\n",
       " \"'s\",\n",
       " 'birthday',\n",
       " 'today',\n",
       " '!',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'unhappi',\n",
       " 'whi',\n",
       " 'alway',\n",
       " 'taken',\n",
       " 'grant',\n",
       " 'eversinc',\n",
       " 'unhappi',\n",
       " 'ah',\n",
       " 'alright',\n",
       " '%',\n",
       " '27t',\n",
       " 'know',\n",
       " 'saw',\n",
       " 'comment',\n",
       " 'yet',\n",
       " 'camera',\n",
       " 'shoot',\n",
       " 'flip',\n",
       " 'scr',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'loui',\n",
       " \"'\",\n",
       " 'tweet',\n",
       " 'unhappi',\n",
       " 'koala',\n",
       " 'die',\n",
       " 'thirst',\n",
       " \"'s\",\n",
       " 'us',\n",
       " 'unhappi',\n",
       " 'okay',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'shut',\n",
       " '.',\n",
       " 'instant',\n",
       " 'messag',\n",
       " 'mad',\n",
       " 'lot',\n",
       " 'peopl',\n",
       " 'veri',\n",
       " 'flaw',\n",
       " 'opinion',\n",
       " 'mental',\n",
       " 'health',\n",
       " '(',\n",
       " 'mine',\n",
       " ')',\n",
       " 'show',\n",
       " 'unhappi',\n",
       " 'pamura',\n",
       " 'isa',\n",
       " 'moment',\n",
       " 'want',\n",
       " 'explod',\n",
       " 'like',\n",
       " 'grenad',\n",
       " 'point',\n",
       " 'peopl',\n",
       " 'die',\n",
       " '.',\n",
       " 'sad',\n",
       " 'yg',\n",
       " 'sent',\n",
       " 'mcd',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'trophi',\n",
       " 'unhappi',\n",
       " 'anyway',\n",
       " 'realli',\n",
       " 'want',\n",
       " 'one',\n",
       " 'icon',\n",
       " 'jimin',\n",
       " 'stripe',\n",
       " 'turtleneck',\n",
       " 'shirt',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " 'want',\n",
       " 'spoon',\n",
       " 'i',\n",
       " 'cant',\n",
       " 'go',\n",
       " 'unhappi',\n",
       " 'honestli',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'messi',\n",
       " 'break',\n",
       " 'unhappi',\n",
       " 'make',\n",
       " 'sad',\n",
       " 'unhappi',\n",
       " 'look',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " 'went',\n",
       " 'seaworld',\n",
       " 'yg',\n",
       " 'sent',\n",
       " 'mcd',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'trophi',\n",
       " 'unhappi',\n",
       " 'anyway',\n",
       " '.1',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'rocki',\n",
       " 'post',\n",
       " 'unhappi',\n",
       " 'hey',\n",
       " 'toni',\n",
       " 'oh',\n",
       " 'unhappi',\n",
       " 'could',\n",
       " 'pleas',\n",
       " 'tell',\n",
       " 'littl',\n",
       " 'issu',\n",
       " '?',\n",
       " 'im',\n",
       " 'follow',\n",
       " 'youd',\n",
       " 'prefer',\n",
       " 'dm',\n",
       " '.',\n",
       " 'amanda',\n",
       " 'love',\n",
       " 'mason',\n",
       " 'miss',\n",
       " 'mason',\n",
       " 'unhappi',\n",
       " 'cold',\n",
       " 'mother',\n",
       " 'crusher',\n",
       " 'right',\n",
       " '.',\n",
       " 'near',\n",
       " 'end',\n",
       " 'april',\n",
       " '.',\n",
       " 'sad',\n",
       " '%',\n",
       " '27t',\n",
       " 'talk',\n",
       " 'anymor',\n",
       " 'like',\n",
       " 'use',\n",
       " 'unhappi',\n",
       " 'yg',\n",
       " 'sent',\n",
       " 'mcd',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'trophi',\n",
       " 'unhappi',\n",
       " 'anyway',\n",
       " '.2',\n",
       " 'miss',\n",
       " 'bike',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'big',\n",
       " 'brother',\n",
       " 'unhappi',\n",
       " '6',\n",
       " 'day',\n",
       " 'camp',\n",
       " 'haiss',\n",
       " 'miss',\n",
       " 'lot',\n",
       " 'unhappi',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'rain',\n",
       " 'hard',\n",
       " 'unhappi',\n",
       " 'am',\n",
       " 'bore',\n",
       " 'kandowiandg',\n",
       " 'i',\n",
       " 'ando',\n",
       " 'plaand',\n",
       " 'today',\n",
       " 'makiandg',\n",
       " 'eveand',\n",
       " 'bore',\n",
       " 'unhappi',\n",
       " 'oh',\n",
       " 'god',\n",
       " 'lauri',\n",
       " 'penni',\n",
       " 'unhappi',\n",
       " 'say',\n",
       " 'hi',\n",
       " 'mekisha',\n",
       " '?',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " 'never',\n",
       " 'draw',\n",
       " 'unhappi',\n",
       " 'cleantha',\n",
       " 'visual',\n",
       " 'studio',\n",
       " 'instal',\n",
       " '-',\n",
       " '89',\n",
       " '%',\n",
       " '..',\n",
       " 'bsod',\n",
       " 'come',\n",
       " 'suddenli',\n",
       " 'unhappi',\n",
       " 'want',\n",
       " 'make',\n",
       " 'waffl',\n",
       " 'unhappi',\n",
       " 'so',\n",
       " 'sad',\n",
       " 'unhappi',\n",
       " 'cri',\n",
       " 'muh',\n",
       " 'feel',\n",
       " 'look',\n",
       " 'like',\n",
       " 'someth',\n",
       " 'ignor',\n",
       " 'xd',\n",
       " 'kart',\n",
       " 'race',\n",
       " '?',\n",
       " 'unhappi',\n",
       " 'ate',\n",
       " 'jenna',\n",
       " 'block',\n",
       " '?',\n",
       " 'unhappi',\n",
       " 'my',\n",
       " 'bed',\n",
       " 'comfort',\n",
       " 'i',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'get',\n",
       " 'unhappi',\n",
       " 'is',\n",
       " 'store',\n",
       " 'still',\n",
       " 'use',\n",
       " '?',\n",
       " 'if',\n",
       " 'i',\n",
       " 'sincer',\n",
       " 'hope',\n",
       " 'mani',\n",
       " 'priceless',\n",
       " 'antiqu',\n",
       " 'destroy',\n",
       " '.',\n",
       " 'reg',\n",
       " 'astaga',\n",
       " 'unhappi',\n",
       " '/',\n",
       " '?',\n",
       " 'i',\n",
       " 'want',\n",
       " 'puppi',\n",
       " 'unhappi',\n",
       " 'work',\n",
       " 'unhappi',\n",
       " \"'ll\",\n",
       " 'see',\n",
       " 'tomorrow',\n",
       " '!',\n",
       " '!',\n",
       " 'happi',\n",
       " 'weed',\n",
       " 'day',\n",
       " 'without',\n",
       " 'anymor',\n",
       " 'unhappi',\n",
       " 'favourit',\n",
       " 'lipstick',\n",
       " 'hilang',\n",
       " 'cri',\n",
       " 'time',\n",
       " 'fli',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'believ',\n",
       " 'year',\n",
       " 'next',\n",
       " 'year',\n",
       " 'unhappi',\n",
       " 'we',\n",
       " \"'re\",\n",
       " 'becom',\n",
       " 'old',\n",
       " 'hahahah',\n",
       " ':',\n",
       " 'v',\n",
       " 'the',\n",
       " 'new',\n",
       " 'twitter',\n",
       " 'repli',\n",
       " 'view',\n",
       " 'confus',\n",
       " '...',\n",
       " 'like',\n",
       " 'i',\n",
       " 'capitalis',\n",
       " 'repli',\n",
       " 'peopl',\n",
       " '?',\n",
       " 'unhappi',\n",
       " 'whaddup',\n",
       " '.',\n",
       " 'me',\n",
       " 'cri',\n",
       " 'unhappi',\n",
       " 'everi',\n",
       " 'time',\n",
       " 'laugh',\n",
       " 'ass',\n",
       " \"'s\",\n",
       " 'sell',\n",
       " 'armi',\n",
       " 'bomb',\n",
       " 'ver',\n",
       " '2',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " 'meet',\n",
       " 'sad',\n",
       " 'yeesh',\n",
       " 'unhappi',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'fairli',\n",
       " 'warm',\n",
       " '.',\n",
       " 'easter',\n",
       " 'flown',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'readi',\n",
       " 'give',\n",
       " 'home',\n",
       " 'luxuri',\n",
       " 'like',\n",
       " 'brand',\n",
       " 'cereal',\n",
       " 'ohnoo',\n",
       " 'unhappi',\n",
       " 'unhappi',\n",
       " 'hope',\n",
       " 'recuper',\n",
       " 'sooner',\n",
       " '!',\n",
       " '!',\n",
       " 'her',\n",
       " 'back',\n",
       " 'unhappi',\n",
       " 'give',\n",
       " 'chanc',\n",
       " 'west',\n",
       " 'server',\n",
       " 'unhappi',\n",
       " 'go',\n",
       " 'yesterday',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " 'agre',\n",
       " '.',\n",
       " 'my',\n",
       " 'issu',\n",
       " 'would',\n",
       " 'paid',\n",
       " 'somehow',\n",
       " '.',\n",
       " 'i',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'see',\n",
       " 'number',\n",
       " 'ad',\n",
       " '.',\n",
       " 'sad',\n",
       " 'i',\n",
       " 'want',\n",
       " 'drink',\n",
       " 'cigarett',\n",
       " 'unhappi',\n",
       " 'oh',\n",
       " 'minc',\n",
       " 'unhappi',\n",
       " 'the',\n",
       " 'manifesto',\n",
       " 'nick',\n",
       " '?',\n",
       " 'might',\n",
       " 'deliv',\n",
       " 'when',\n",
       " 'time',\n",
       " 'demand',\n",
       " '.',\n",
       " 'stori',\n",
       " 'life',\n",
       " '-',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'okay',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'accept',\n",
       " 'way',\n",
       " 'unhappi',\n",
       " 'peopl',\n",
       " 'abus',\n",
       " 'anim',\n",
       " 'unhappi',\n",
       " \"'re\",\n",
       " 'loyal',\n",
       " \"'m\",\n",
       " 'actual',\n",
       " 'cri',\n",
       " 'type',\n",
       " 'tweet',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'take',\n",
       " 'anymor',\n",
       " '..',\n",
       " 'applicablehulog',\n",
       " 'applic',\n",
       " 'knock',\n",
       " 'unhappi',\n",
       " 'imagin',\n",
       " 'win',\n",
       " 'next',\n",
       " 'time',\n",
       " 'unhappi',\n",
       " 'unhappi',\n",
       " 'same',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " 'need',\n",
       " 'cue',\n",
       " 'someth',\n",
       " 'would',\n",
       " 'make',\n",
       " 'smile',\n",
       " '....',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'wait',\n",
       " '...',\n",
       " ':',\n",
       " '(',\n",
       " 'yg',\n",
       " 'sent',\n",
       " 'mcd',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'trophi',\n",
       " 'unhappi',\n",
       " 'anyway',\n",
       " '.3',\n",
       " 'yg',\n",
       " 'sent',\n",
       " 'mcd',\n",
       " '.',\n",
       " 'i',\n",
       " 'want',\n",
       " 'see',\n",
       " 'hold',\n",
       " 'trophi',\n",
       " 'unhappi',\n",
       " 'anyway',\n",
       " '.4',\n",
       " 'noseble',\n",
       " 'get',\n",
       " 'outta',\n",
       " 'hand',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " 'perfectli',\n",
       " 'happi',\n",
       " 'singl',\n",
       " '..',\n",
       " 'until',\n",
       " 'i',\n",
       " 'see',\n",
       " 'happi',\n",
       " 'coupl',\n",
       " ':',\n",
       " '(',\n",
       " 'kiss',\n",
       " 'thefashionicon',\n",
       " 'dude',\n",
       " 'i',\n",
       " 'want',\n",
       " 'sleep',\n",
       " 'unhappi',\n",
       " 'feel',\n",
       " 'fuck',\n",
       " 'shit',\n",
       " 'today',\n",
       " 'unhappi',\n",
       " 'wont',\n",
       " 'abl',\n",
       " 'stream',\n",
       " 'tonight',\n",
       " \"'m\",\n",
       " 'sorri',\n",
       " 'guy',\n",
       " 'unhappi',\n",
       " 'i',\n",
       " 'face',\n",
       " 'swap',\n",
       " 'cat',\n",
       " 'dog',\n",
       " \"'s\",\n",
       " 'realli',\n",
       " 'upset',\n",
       " 'unhappi',\n",
       " 'system',\n",
       " 'recogn',\n",
       " 'space',\n",
       " 'last',\n",
       " 'name',\n",
       " '2nd',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df = pd.read_csv('data/processedNegative.csv').T.reset_index()\n",
    "neg_text = \" \".join([tweet[0] for tweet in neg_df.values.tolist()])\n",
    "neg_tokens = [word for word in word_tokenize(neg_text) if not word in stopwords.words('english')]\n",
    "ps = PorterStemmer()\n",
    "neg_stem = [ps.stem(word) for word in neg_tokens]\n",
    "neg_stem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Функция, которая создасть набор данных для обучения моделей<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_file_to_df(file_name):\n",
    "    neg_fn, neut_fn, pos_fn = file_name\n",
    "\n",
    "    neg_df = pd.read_csv(neg_fn).T.reset_index()\n",
    "    neut_df = pd.read_csv(neut_fn).T.reset_index()\n",
    "    pos_df = pd.read_csv(pos_fn).T.reset_index()\n",
    "    \n",
    "    neg_text = \" \".join([tweet[0] for tweet in neg_df.values.tolist()])\n",
    "    neut_text = \" \".join([tweet[0] for tweet in neut_df.values.tolist()])\n",
    "    pos_text = \" \".join([tweet[0] for tweet in pos_df.values.tolist()])\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    neg_words = Counter([ps.stem(word) for word in word_tokenize(neg_text) if not word in stopwords.words('english')])\n",
    "    neut_words = Counter([ps.stem(word) for word in word_tokenize(neut_text) if not word in stopwords.words('english')])\n",
    "    pos_words = Counter([ps.stem(word) for word in word_tokenize(pos_text) if not word in stopwords.words('english')])\n",
    "    \n",
    "    unic_words = list(set(neg_words.keys()) | set(neut_words.keys()) | set(pos_words.keys()))\n",
    "\n",
    "    neg_exist_index = 0\n",
    "    neut_exist_index = 1\n",
    "    pos_exist_index = 2\n",
    "    neg_count_index = 3\n",
    "    neut_count_index = 4\n",
    "    pos_count_index = 5\n",
    "    word_count_index = 6\n",
    "    neg_tfidf_index = 7\n",
    "    neut_tfidf_index = 8\n",
    "    pos_tfidf_index = 9\n",
    "\n",
    "    df = np.zeros((len(unic_words), 10))\n",
    "    for i, word in enumerate(unic_words):\n",
    "        if word in neg_words.keys():\n",
    "            df[i,neg_exist_index] = 1\n",
    "            df[i,neg_count_index] = neg_words[word]\n",
    "        if word in neut_words.keys():\n",
    "            df[i,neut_exist_index] = 1\n",
    "            df[i,neut_count_index] = neut_words[word]\n",
    "        if word in pos_words.keys():\n",
    "            df[i,pos_exist_index] = 1\n",
    "            df[i,pos_count_index] = pos_words[word]\n",
    "\n",
    "    df[:,word_count_index] = df[:,neg_count_index] + df[:,neut_count_index] + df[:,pos_count_index]\n",
    "    df[:,neg_tfidf_index] = df[:,neg_count_index] / df[:,word_count_index]\n",
    "    df[:,neut_tfidf_index] = df[:,neut_count_index] / df[:,word_count_index]\n",
    "    df[:,pos_tfidf_index] = df[:,pos_count_index] / df[:,word_count_index]\n",
    "\n",
    "    stem_df = pd.DataFrame(df, columns=[\n",
    "        'Negative', 'Neutral', 'Positive',\n",
    "        'Negative counts', 'Neutral counts', 'Positive counts', 'Word counts',\n",
    "        'Negative TFIDF', 'Neutral TFIDF', 'Positive TFIDF'])\n",
    "    stem_df[\"word\"] = unic_words\n",
    "    return stem_df, unic_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Узнаем, как называются остальные файлы, содержащие исходный набор данных<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mp00_tweets.zip\u001b[0m*         \u001b[01;32mprocessedNeutral.csv\u001b[0m*\n",
      "\u001b[01;32mprocessedNegative.csv\u001b[0m*  \u001b[01;32mprocessedPositive.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Создадим набор данных для обучения<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative counts</th>\n",
       "      <th>Neutral counts</th>\n",
       "      <th>Positive counts</th>\n",
       "      <th>Word counts</th>\n",
       "      <th>Negative TFIDF</th>\n",
       "      <th>Neutral TFIDF</th>\n",
       "      <th>Positive TFIDF</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>s.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>goldi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>such</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>scrobbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>tanner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ttwebonlinefe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>happy.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>japanes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>shoehorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5553 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Negative  Neutral  Positive  Negative counts  Neutral counts  \\\n",
       "0          0.0      0.0       1.0              0.0             0.0   \n",
       "1          0.0      0.0       1.0              0.0             0.0   \n",
       "2          1.0      0.0       0.0              1.0             0.0   \n",
       "3          1.0      0.0       0.0              1.0             0.0   \n",
       "4          0.0      0.0       1.0              0.0             0.0   \n",
       "...        ...      ...       ...              ...             ...   \n",
       "5548       0.0      1.0       0.0              0.0             1.0   \n",
       "5549       0.0      0.0       1.0              0.0             0.0   \n",
       "5550       1.0      0.0       1.0              1.0             0.0   \n",
       "5551       1.0      0.0       0.0              2.0             0.0   \n",
       "5552       1.0      0.0       1.0              2.0             0.0   \n",
       "\n",
       "      Positive counts  Word counts  Negative TFIDF  Neutral TFIDF  \\\n",
       "0                 1.0          1.0        0.000000            0.0   \n",
       "1                 1.0          1.0        0.000000            0.0   \n",
       "2                 0.0          1.0        1.000000            0.0   \n",
       "3                 0.0          1.0        1.000000            0.0   \n",
       "4                 2.0          2.0        0.000000            0.0   \n",
       "...               ...          ...             ...            ...   \n",
       "5548              0.0          1.0        0.000000            1.0   \n",
       "5549              4.0          4.0        0.000000            0.0   \n",
       "5550              2.0          3.0        0.333333            0.0   \n",
       "5551              0.0          2.0        1.000000            0.0   \n",
       "5552              2.0          4.0        0.500000            0.0   \n",
       "\n",
       "      Positive TFIDF           word  \n",
       "0           1.000000            s.1  \n",
       "1           1.000000          goldi  \n",
       "2           0.000000           such  \n",
       "3           0.000000        scrobbl  \n",
       "4           1.000000         tanner  \n",
       "...              ...            ...  \n",
       "5548        0.000000  ttwebonlinefe  \n",
       "5549        1.000000        happy.4  \n",
       "5550        0.666667        japanes  \n",
       "5551        0.000000       shoehorn  \n",
       "5552        0.500000              +  \n",
       "\n",
       "[5553 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = ('data/processedNegative.csv', 'data/processedNeutral.csv', 'data/processedPositive.csv')\n",
    "stem_df, unic_words = stem_file_to_df(file_names)\n",
    "stem_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Узнаем полученную точность модели<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "word_exist_accuracy_score = model_selection_word_exist(stem_df, unic_words)\n",
    "word_count_accuracy_score = model_selection_word_count(stem_df, unic_words)\n",
    "tfidf_accuracy_score = model_selection_tfidf(stem_df, unic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score by word exist: 0.5220522052205221\n",
      "Accuracy score by word count: 0.9747974797479748\n",
      "Fccuracy score by tfidf: 0.5148514851485149\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Accuracy score by word exist: {word_exist_accuracy_score}\n",
    "Accuracy score by word count: {word_count_accuracy_score}\n",
    "Fccuracy score by tfidf: {tfidf_accuracy_score}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
