{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Опечатки<h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Метод берет список слов с ошибками и предлагает правильное слово для каждого неправильного слова. Он пытается найти в списке правильных вариантов написания слово с наименьшим расстоянием и той же начальной буквой, что и слово с ошибкой. Затем он возвращает слово, которое соответствует заданным критериям. Методы можно различать на основе меры расстояния, которую они используют для поиска ближайшего слова. В качестве словаря правильных слов используется пакет «words» от nltk.<h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Импортируем библиотеки<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/val/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/val/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from model_selction import model_selection_word_count, model_selection_word_exist, model_selection_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mp00_tweets.zip\u001b[0m*         \u001b[01;32mprocessedNeutral.csv\u001b[0m*\n",
      "\u001b[01;32mprocessedNegative.csv\u001b[0m*  \u001b[01;32mprocessedPositive.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>В качестве примера рассмотрим содержимое файла 'processedNegative.csv' после применения метода<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ho',\n",
       " 'unhappy',\n",
       " 'dogs',\n",
       " 'like',\n",
       " 'though',\n",
       " 'talking',\n",
       " 'driver',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'said',\n",
       " \"'d\",\n",
       " 'love',\n",
       " 'go',\n",
       " 'Newar',\n",
       " 'Yorker',\n",
       " 'since',\n",
       " 'Triumph',\n",
       " \"'s\",\n",
       " 'probably',\n",
       " 'Doeg',\n",
       " 'anybody',\n",
       " 'know',\n",
       " 'Rand',\n",
       " \"'s\",\n",
       " 'likely',\n",
       " 'fall',\n",
       " 'dollar',\n",
       " '?',\n",
       " 'I',\n",
       " 'got',\n",
       " 'money',\n",
       " 'I',\n",
       " 'need',\n",
       " 'change',\n",
       " 'R',\n",
       " 'keep',\n",
       " 'getting',\n",
       " 'strong',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'going',\n",
       " 'gig',\n",
       " 'Liverpudlian',\n",
       " 'unhappy',\n",
       " 'Theresa',\n",
       " 'new',\n",
       " 'Ricciales',\n",
       " 'tonight',\n",
       " '?',\n",
       " 'unhappy',\n",
       " \"'s\",\n",
       " 'A',\n",
       " '*',\n",
       " 'dye',\n",
       " 'guy',\n",
       " 'pop',\n",
       " 'Asian',\n",
       " 'translator',\n",
       " \"'ll\",\n",
       " 'prob',\n",
       " 'go',\n",
       " 'around',\n",
       " 'Aus',\n",
       " 'unhappy',\n",
       " 'Whig',\n",
       " \"'s\",\n",
       " 'chair',\n",
       " \"'re\",\n",
       " 'sitting',\n",
       " '?',\n",
       " 'Isis',\n",
       " 'I',\n",
       " 'find',\n",
       " '.',\n",
       " 'Everyman',\n",
       " 'know',\n",
       " '.',\n",
       " 'Yomud',\n",
       " \"'ve\",\n",
       " 'shamed',\n",
       " 'pu',\n",
       " 'n',\n",
       " 'like',\n",
       " 'jittery',\n",
       " 'caffeine',\n",
       " 'make',\n",
       " 'sad',\n",
       " 'Mya',\n",
       " 'area',\n",
       " \"'s\",\n",
       " 'list',\n",
       " 'unhappy',\n",
       " 'think',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'go',\n",
       " 'Libby',\n",
       " 'anyway',\n",
       " 'I',\n",
       " 'want',\n",
       " 'fun',\n",
       " 'plan',\n",
       " 'weekend',\n",
       " 'unhappy',\n",
       " 'Wend',\n",
       " 'notice',\n",
       " '.',\n",
       " 'unhappy',\n",
       " '?',\n",
       " 'Ah',\n",
       " '!',\n",
       " 'Yomud',\n",
       " 'recognize',\n",
       " 'L',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'Cinemascope',\n",
       " 'show',\n",
       " 'B',\n",
       " 'track',\n",
       " 'record',\n",
       " 'getting',\n",
       " 'canceler',\n",
       " 'unhappy',\n",
       " 'Errantia',\n",
       " 'dude',\n",
       " '....',\n",
       " 'The',\n",
       " \"'re\",\n",
       " 'gone',\n",
       " 'unhappy',\n",
       " 'Askr',\n",
       " 'league',\n",
       " 'member',\n",
       " 'check',\n",
       " 'guy',\n",
       " 'go',\n",
       " 'No',\n",
       " 'sad',\n",
       " 'Whig',\n",
       " 'would',\n",
       " 'Harvey',\n",
       " 'going',\n",
       " 'prison',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'Ming',\n",
       " 'crying',\n",
       " 'Sepioidea',\n",
       " 'area',\n",
       " '.',\n",
       " 'Beck',\n",
       " 'depend',\n",
       " 'promotion',\n",
       " 'waste',\n",
       " 'handwork',\n",
       " 'team',\n",
       " 'I',\n",
       " 'thought',\n",
       " \"'ll\",\n",
       " 'save',\n",
       " 'crying',\n",
       " 'major',\n",
       " 'waffle',\n",
       " 'craving',\n",
       " 'right',\n",
       " 'sad',\n",
       " 'cant',\n",
       " 'speak',\n",
       " 'japan',\n",
       " ':',\n",
       " ':',\n",
       " '(',\n",
       " 'people',\n",
       " 'stuff',\n",
       " 'like',\n",
       " 'unhappy',\n",
       " 'please',\n",
       " 'stop',\n",
       " 'confining',\n",
       " 'animal',\n",
       " 'zoo',\n",
       " 'unhappy',\n",
       " 'Felis',\n",
       " 'like',\n",
       " 'telling',\n",
       " 'get',\n",
       " 'fuci',\n",
       " 'social',\n",
       " 'media',\n",
       " 'byous',\n",
       " 'also',\n",
       " 'feel',\n",
       " 'really',\n",
       " 'mean',\n",
       " 'unhappy',\n",
       " 'silence',\n",
       " 'love',\n",
       " 'hope',\n",
       " 'oka',\n",
       " 'miss',\n",
       " 'huh',\n",
       " 'busy',\n",
       " 'unhappy',\n",
       " 'extended',\n",
       " 'family',\n",
       " '.',\n",
       " '12',\n",
       " 'popple',\n",
       " 'wanter',\n",
       " 'show',\n",
       " 'Ohio',\n",
       " 'Mya',\n",
       " 'Gi',\n",
       " 'dor',\n",
       " 'playingly',\n",
       " 'game',\n",
       " 'got',\n",
       " 'delete',\n",
       " 'unhappy',\n",
       " 'Dob',\n",
       " 'n',\n",
       " 'unhappy',\n",
       " 'Jamie',\n",
       " 'please',\n",
       " 'reset',\n",
       " 'C',\n",
       " 'grandfilial',\n",
       " 'serve',\n",
       " '...',\n",
       " 'administrator',\n",
       " 'respond',\n",
       " 'unhappy',\n",
       " 'nook',\n",
       " 'Y',\n",
       " 'G',\n",
       " 'N',\n",
       " 'M',\n",
       " 'T',\n",
       " 'B',\n",
       " 'unhappy',\n",
       " 'T',\n",
       " 'C',\n",
       " 'A',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'I',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'vote',\n",
       " 'unhappy',\n",
       " 'instant',\n",
       " 'message',\n",
       " 'jealous',\n",
       " 'oka',\n",
       " 'unhappy',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'hah',\n",
       " 'brunt',\n",
       " 'wait',\n",
       " 'final',\n",
       " 'first',\n",
       " \"'m\",\n",
       " 'enlist',\n",
       " 'please',\n",
       " 'turn',\n",
       " 'like',\n",
       " 'unhappy',\n",
       " 'unhappy',\n",
       " 'come',\n",
       " 'people',\n",
       " 'like',\n",
       " 'childrenite',\n",
       " \"'s\",\n",
       " 'state',\n",
       " 'intervention',\n",
       " 'Ophiuchid',\n",
       " 'unhappy',\n",
       " 'Helen',\n",
       " '...',\n",
       " 'I',\n",
       " 'want',\n",
       " 'stop',\n",
       " 'teeting',\n",
       " '.',\n",
       " 'Alle',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'endless',\n",
       " 'suffering',\n",
       " 'pain',\n",
       " '.',\n",
       " 'I',\n",
       " 'tried',\n",
       " 'deactivate',\n",
       " 'many',\n",
       " 'times',\n",
       " '...',\n",
       " 'Savery',\n",
       " '...',\n",
       " 'unhappy',\n",
       " 'For',\n",
       " 'askingly',\n",
       " 'application',\n",
       " 'Kanaka',\n",
       " 'Kanji',\n",
       " 'Funtumia',\n",
       " '!',\n",
       " 'Saad',\n",
       " 'look',\n",
       " 'like',\n",
       " '11',\n",
       " 'due',\n",
       " 'kill',\n",
       " 'unhappy',\n",
       " 'Yeshibah',\n",
       " 'update',\n",
       " '16.04',\n",
       " 'froze',\n",
       " 'times',\n",
       " '.',\n",
       " 'The',\n",
       " 'went',\n",
       " '16.10',\n",
       " 'froze',\n",
       " 'mid',\n",
       " 'install',\n",
       " '.',\n",
       " 'Wagnerite',\n",
       " '3hrs',\n",
       " \"'d\",\n",
       " 'pull',\n",
       " 'plug',\n",
       " 'crying',\n",
       " 'Shardana',\n",
       " 'Zabaean',\n",
       " 'Anoplotherium',\n",
       " 'A',\n",
       " \"'s\",\n",
       " 'way',\n",
       " '!',\n",
       " 'I',\n",
       " 'wish',\n",
       " 'Sri',\n",
       " 'Sir',\n",
       " 'start',\n",
       " 'sighing',\n",
       " 'good',\n",
       " 'movie',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'want',\n",
       " 'Jabberwock',\n",
       " 'crying',\n",
       " 'sociocrat',\n",
       " 'full',\n",
       " 'raid',\n",
       " 'gear',\n",
       " 'sad',\n",
       " 'Wend',\n",
       " 'say',\n",
       " 'hi',\n",
       " 'sunshine',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'feel',\n",
       " 'bad',\n",
       " 'A',\n",
       " 'unhappy',\n",
       " \"'s\",\n",
       " 'getting',\n",
       " 'harder',\n",
       " 'harder',\n",
       " 'stay',\n",
       " 'unhappy',\n",
       " 'Hispa',\n",
       " 'face',\n",
       " 'look',\n",
       " 'bloated',\n",
       " 'unhappy',\n",
       " 'baby',\n",
       " 'get',\n",
       " 'well',\n",
       " 'soon',\n",
       " 'fuci',\n",
       " '.',\n",
       " 'tried',\n",
       " 'chang',\n",
       " 'setting',\n",
       " 'still',\n",
       " 'indicia',\n",
       " '.',\n",
       " 'unhappy',\n",
       " 'talking',\n",
       " 'driver',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'said',\n",
       " \"'d\",\n",
       " 'love',\n",
       " 'go',\n",
       " 'Newar',\n",
       " 'Yorker',\n",
       " 'since',\n",
       " 'Triumph',\n",
       " \"'s\",\n",
       " 'probably',\n",
       " 'Leto',\n",
       " \"'s\",\n",
       " 'forget',\n",
       " \"'s\",\n",
       " 'also',\n",
       " 'Gabriel',\n",
       " 'Tema',\n",
       " 'Whit',\n",
       " \"'s\",\n",
       " 'birthday',\n",
       " 'today',\n",
       " '!',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'unhappy',\n",
       " 'Whig',\n",
       " 'always',\n",
       " 'taken',\n",
       " 'grantedly',\n",
       " 'eversive',\n",
       " 'unhappy',\n",
       " 'Ah',\n",
       " 'alright',\n",
       " '%',\n",
       " '27t',\n",
       " 'know',\n",
       " 'saw',\n",
       " 'comment',\n",
       " 'yet',\n",
       " 'camera',\n",
       " 'shooting',\n",
       " 'flip',\n",
       " 'scry',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'Louis',\n",
       " \"'\",\n",
       " 'tweet',\n",
       " 'unhappy',\n",
       " 'Koasati',\n",
       " 'dying',\n",
       " 'thirst',\n",
       " \"'s\",\n",
       " 'us',\n",
       " 'unhappy',\n",
       " 'oka',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'shut',\n",
       " '.',\n",
       " 'instant',\n",
       " 'message',\n",
       " 'mad',\n",
       " 'lots',\n",
       " 'people',\n",
       " 'V',\n",
       " 'flawed',\n",
       " 'opinion',\n",
       " 'mental',\n",
       " 'health',\n",
       " '(',\n",
       " 'mine',\n",
       " ')',\n",
       " 'show',\n",
       " 'unhappy',\n",
       " 'param',\n",
       " 'moment',\n",
       " 'want',\n",
       " 'explode',\n",
       " 'like',\n",
       " 'grenade',\n",
       " 'point',\n",
       " 'people',\n",
       " 'die',\n",
       " '.',\n",
       " 'sad',\n",
       " 'Y',\n",
       " 'sent',\n",
       " 'M',\n",
       " '.',\n",
       " 'I',\n",
       " 'want',\n",
       " 'see',\n",
       " 'holding',\n",
       " 'trophy',\n",
       " 'unhappy',\n",
       " 'anyways',\n",
       " 'really',\n",
       " 'want',\n",
       " 'one',\n",
       " 'iconic',\n",
       " 'jimp',\n",
       " 'stripe',\n",
       " 'turtle',\n",
       " 'shirt',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'want',\n",
       " 'spoon',\n",
       " 'I',\n",
       " 'cant',\n",
       " 'go',\n",
       " 'unhappy',\n",
       " 'honestly',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'messy',\n",
       " 'break',\n",
       " 'unhappy',\n",
       " 'Maku',\n",
       " 'sad',\n",
       " 'unhappy',\n",
       " 'looker',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'went',\n",
       " 'Sean',\n",
       " 'Y',\n",
       " 'sent',\n",
       " 'M',\n",
       " '.',\n",
       " 'I',\n",
       " 'want',\n",
       " 'see',\n",
       " 'holding',\n",
       " 'trophy',\n",
       " 'unhappy',\n",
       " 'anyways',\n",
       " '.1',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'Rockies',\n",
       " 'post',\n",
       " 'unhappy',\n",
       " 'Hehe',\n",
       " 'Tony',\n",
       " 'oh',\n",
       " 'unhappy',\n",
       " 'Cours',\n",
       " 'please',\n",
       " 'tell',\n",
       " 'little',\n",
       " 'issue',\n",
       " '?',\n",
       " 'Ima',\n",
       " 'following',\n",
       " 'youd',\n",
       " 'prefer',\n",
       " 'D',\n",
       " '.',\n",
       " 'Amanda',\n",
       " 'love',\n",
       " 'mason',\n",
       " 'miss',\n",
       " 'mason',\n",
       " 'unhappy',\n",
       " 'Cola',\n",
       " 'mother',\n",
       " 'crusher',\n",
       " 'right',\n",
       " '.',\n",
       " 'Nerine',\n",
       " 'end',\n",
       " 'April',\n",
       " '.',\n",
       " 'sad',\n",
       " '%',\n",
       " '27t',\n",
       " 'talk',\n",
       " 'ananym',\n",
       " 'like',\n",
       " 'used',\n",
       " 'unhappy',\n",
       " 'Y',\n",
       " 'sent',\n",
       " 'M',\n",
       " '.',\n",
       " 'I',\n",
       " 'want',\n",
       " 'see',\n",
       " 'holding',\n",
       " 'trophy',\n",
       " 'unhappy',\n",
       " 'anyways',\n",
       " '.2',\n",
       " 'miss',\n",
       " 'bikini',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'miss',\n",
       " 'Bim',\n",
       " 'brother',\n",
       " 'unhappy',\n",
       " '6',\n",
       " 'days',\n",
       " 'camp',\n",
       " 'haik',\n",
       " 'miss',\n",
       " 'lot',\n",
       " 'unhappy',\n",
       " 'Ita',\n",
       " \"'s\",\n",
       " 'rain',\n",
       " 'hard',\n",
       " 'unhappy',\n",
       " 'Ami',\n",
       " 'bore',\n",
       " 'kandol',\n",
       " 'I',\n",
       " 'plan',\n",
       " 'today',\n",
       " 'manakin',\n",
       " 'ean',\n",
       " 'bore',\n",
       " 'unhappy',\n",
       " 'oh',\n",
       " 'god',\n",
       " 'lauric',\n",
       " 'penny',\n",
       " 'unhappy',\n",
       " 'say',\n",
       " 'Hima',\n",
       " 'Medish',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'never',\n",
       " 'draw',\n",
       " 'unhappy',\n",
       " 'Cyclanthales',\n",
       " 'Vishal',\n",
       " 'Studite',\n",
       " 'Italianation',\n",
       " '-',\n",
       " '89',\n",
       " '%',\n",
       " '..',\n",
       " 'B',\n",
       " 'comes',\n",
       " 'suddenly',\n",
       " 'unhappy',\n",
       " 'want',\n",
       " 'make',\n",
       " 'waffle',\n",
       " 'unhappy',\n",
       " 'Sol',\n",
       " 'sad',\n",
       " 'unhappy',\n",
       " 'crying',\n",
       " 'mu',\n",
       " 'feel',\n",
       " 'Lo',\n",
       " 'like',\n",
       " 'something',\n",
       " 'ignore',\n",
       " 'x',\n",
       " 'Kari',\n",
       " 'Rauraci',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'ate',\n",
       " 'jenna',\n",
       " 'blocked',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'Mya',\n",
       " 'bed',\n",
       " 'comfortable',\n",
       " 'I',\n",
       " 'n',\n",
       " 'want',\n",
       " 'get',\n",
       " 'unhappy',\n",
       " 'Isis',\n",
       " 'store',\n",
       " 'still',\n",
       " 'use',\n",
       " '?',\n",
       " 'Ifugao',\n",
       " 'I',\n",
       " 'sincerely',\n",
       " 'hope',\n",
       " 'many',\n",
       " 'priceless',\n",
       " 'antique',\n",
       " 'destroyer',\n",
       " '.',\n",
       " 'Regga',\n",
       " 'Astragalus',\n",
       " 'unhappy',\n",
       " '/',\n",
       " '?',\n",
       " 'I',\n",
       " 'want',\n",
       " 'puppy',\n",
       " 'unhappy',\n",
       " 'work',\n",
       " 'unhappy',\n",
       " \"'ll\",\n",
       " 'see',\n",
       " 'tomorrow',\n",
       " '!',\n",
       " '!',\n",
       " 'happy',\n",
       " 'weed',\n",
       " 'day',\n",
       " 'without',\n",
       " 'ananym',\n",
       " 'unhappy',\n",
       " 'favorite',\n",
       " 'lipstick',\n",
       " 'hilding',\n",
       " 'crying',\n",
       " 'Tim',\n",
       " 'flier',\n",
       " 'ca',\n",
       " 'n',\n",
       " 'believe',\n",
       " 'year',\n",
       " 'next',\n",
       " 'year',\n",
       " 'unhappy',\n",
       " 'Wea',\n",
       " \"'re\",\n",
       " 'becoming',\n",
       " 'old',\n",
       " 'H',\n",
       " ':',\n",
       " 'v',\n",
       " 'The',\n",
       " 'new',\n",
       " 'Twi',\n",
       " 'reply',\n",
       " 'view',\n",
       " 'hash',\n",
       " 'confused',\n",
       " '...',\n",
       " 'like',\n",
       " 'I',\n",
       " 'capitalism',\n",
       " 'replier',\n",
       " 'people',\n",
       " '?',\n",
       " 'unhappy',\n",
       " 'Whitechapel',\n",
       " '.',\n",
       " 'Meg',\n",
       " 'crying',\n",
       " 'unhappy',\n",
       " 'every',\n",
       " 'time',\n",
       " 'laughing',\n",
       " 'ass',\n",
       " \"'s\",\n",
       " 'selling',\n",
       " 'army',\n",
       " 'bomb',\n",
       " 'veer',\n",
       " '2',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " 'meet',\n",
       " 'sad',\n",
       " 'Yeshiva',\n",
       " 'unhappy',\n",
       " 'Ita',\n",
       " \"'s\",\n",
       " 'fairly',\n",
       " 'warm',\n",
       " '.',\n",
       " 'easter',\n",
       " 'hash',\n",
       " 'flown',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'ready',\n",
       " 'give',\n",
       " 'home',\n",
       " 'luxurist',\n",
       " 'like',\n",
       " 'branded',\n",
       " 'cereal',\n",
       " 'Ohio',\n",
       " 'unhappy',\n",
       " 'unhappy',\n",
       " 'hope',\n",
       " 'recuperate',\n",
       " 'sooner',\n",
       " '!',\n",
       " '!',\n",
       " 'Herb',\n",
       " 'back',\n",
       " 'unhappy',\n",
       " 'give',\n",
       " 'chance',\n",
       " 'west',\n",
       " 'serve',\n",
       " 'unhappy',\n",
       " 'going',\n",
       " 'yesterday',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'agree',\n",
       " '.',\n",
       " 'Mya',\n",
       " 'issue',\n",
       " 'would',\n",
       " 'hash',\n",
       " 'paik',\n",
       " 'somehow',\n",
       " '.',\n",
       " 'I',\n",
       " 'ca',\n",
       " 'n',\n",
       " 'see',\n",
       " 'number',\n",
       " 'addlings',\n",
       " '.',\n",
       " 'sad',\n",
       " 'I',\n",
       " 'want',\n",
       " 'drink',\n",
       " 'cigarette',\n",
       " 'unhappy',\n",
       " 'Ohio',\n",
       " 'mince',\n",
       " 'unhappy',\n",
       " 'The',\n",
       " 'manifesto',\n",
       " 'Nick',\n",
       " '?',\n",
       " 'Miguel',\n",
       " 'deliver',\n",
       " 'W',\n",
       " 'time',\n",
       " 'demand',\n",
       " '.',\n",
       " 'Stentor',\n",
       " 'life',\n",
       " '-',\n",
       " 'Ita',\n",
       " \"'s\",\n",
       " 'oka',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'accepted',\n",
       " 'way',\n",
       " 'unhappy',\n",
       " 'people',\n",
       " 'abuse',\n",
       " 'animal',\n",
       " 'unhappy',\n",
       " \"'re\",\n",
       " 'loyal',\n",
       " \"'m\",\n",
       " 'actually',\n",
       " 'crying',\n",
       " 'teletyping',\n",
       " 'tweet',\n",
       " 'ca',\n",
       " 'n',\n",
       " 'take',\n",
       " 'ananym',\n",
       " '..',\n",
       " 'applicable',\n",
       " 'applicable',\n",
       " 'knock',\n",
       " 'unhappy',\n",
       " 'imagine',\n",
       " 'win',\n",
       " 'next',\n",
       " 'time',\n",
       " 'unhappy',\n",
       " 'unhappy',\n",
       " 'Sam',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'need',\n",
       " 'Cueva',\n",
       " 'something',\n",
       " 'would',\n",
       " 'make',\n",
       " 'smile',\n",
       " '....',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'waiting',\n",
       " '...',\n",
       " ':',\n",
       " '(',\n",
       " 'Y',\n",
       " 'sent',\n",
       " 'M',\n",
       " '.',\n",
       " 'I',\n",
       " 'want',\n",
       " 'see',\n",
       " 'holding',\n",
       " 'trophy',\n",
       " 'unhappy',\n",
       " 'anyways',\n",
       " '.3',\n",
       " 'Y',\n",
       " 'sent',\n",
       " 'M',\n",
       " '.',\n",
       " 'I',\n",
       " 'want',\n",
       " 'see',\n",
       " 'holding',\n",
       " 'trophy',\n",
       " 'unhappy',\n",
       " 'anyways',\n",
       " '.4',\n",
       " 'nosebleed',\n",
       " 'getting',\n",
       " 'outtalk',\n",
       " 'hand',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'perfectly',\n",
       " 'happy',\n",
       " 'single',\n",
       " '..',\n",
       " 'Umatilla',\n",
       " 'I',\n",
       " 'see',\n",
       " 'happy',\n",
       " 'couple',\n",
       " ':',\n",
       " '(',\n",
       " 'K',\n",
       " 'Theridion',\n",
       " 'Dudleya',\n",
       " 'I',\n",
       " 'want',\n",
       " 'sleep',\n",
       " 'unhappy',\n",
       " 'Felis',\n",
       " 'fackings',\n",
       " 'shita',\n",
       " 'today',\n",
       " 'unhappy',\n",
       " 'wont',\n",
       " 'able',\n",
       " 'stream',\n",
       " 'tonight',\n",
       " \"'m\",\n",
       " 'sorry',\n",
       " 'guy',\n",
       " 'unhappy',\n",
       " 'I',\n",
       " 'face',\n",
       " 'swapper',\n",
       " 'cat',\n",
       " 'dog',\n",
       " \"'s\",\n",
       " 'really',\n",
       " 'upsetting',\n",
       " 'unhappy',\n",
       " 'system',\n",
       " 'recognize',\n",
       " 'space',\n",
       " 'last',\n",
       " 'name',\n",
       " '2nd',\n",
       " 'time',\n",
       " 'unable',\n",
       " 'check',\n",
       " 'forced',\n",
       " 'wait',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df = pd.read_csv('data/processedNegative.csv').T.reset_index()\n",
    "neg_text = \" \".join([tweet[0] for tweet in neg_df.values.tolist()])\n",
    "neg_tokens = word_tokenize(neg_text)\n",
    "correct_words = words.words()\n",
    "neg_spell = list()\n",
    "for word in neg_tokens:\n",
    "    if len(word) > 1:\n",
    "        temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                                set(ngrams(w, 2))),w)\n",
    "                for w in correct_words if w[0]==word[0]]\n",
    "        try:\n",
    "            neg_spell.append(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "        except IndexError:\n",
    "            neg_spell.append(word)\n",
    "    else:\n",
    "        neg_spell.append(word)\n",
    "neg_spell = [word for word in neg_spell if not word in stopwords.words('english')]\n",
    "neg_spell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Функция, которая создасть набор данных для обучения моделей<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_file_to_df(file_name):\n",
    "    neg_fn, neut_fn, pos_fn = file_name\n",
    "\n",
    "    neg_df = pd.read_csv(neg_fn).T.reset_index()\n",
    "    neut_df = pd.read_csv(neut_fn).T.reset_index()\n",
    "    pos_df = pd.read_csv(pos_fn).T.reset_index()\n",
    "    \n",
    "    neg_text = \" \".join([tweet[0] for tweet in neg_df.values.tolist()])\n",
    "    neut_text = \" \".join([tweet[0] for tweet in neut_df.values.tolist()])\n",
    "    pos_text = \" \".join([tweet[0] for tweet in pos_df.values.tolist()])\n",
    "\n",
    "    correct_words = words.words()\n",
    "    \n",
    "    neg_tokens = word_tokenize(neg_text)\n",
    "    neg_spell = list()\n",
    "    for word in neg_tokens:\n",
    "        if len(word) > 1:\n",
    "            temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                                    set(ngrams(w, 2))),w)\n",
    "                    for w in correct_words if w[0]==word[0]]\n",
    "            try:\n",
    "                neg_spell.append(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "            except IndexError:\n",
    "                neg_spell.append(word)\n",
    "        else:\n",
    "            neg_spell.append(word)\n",
    "    neg_spell = [word for word in neg_spell if not word in stopwords.words('english')]\n",
    "\n",
    "    neut_tokens = word_tokenize(neut_text)\n",
    "    neut_spell = list()\n",
    "    for word in neut_tokens:\n",
    "        if len(word) > 1:\n",
    "            temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                                    set(ngrams(w, 2))),w)\n",
    "                    for w in correct_words if w[0]==word[0]]\n",
    "            try:\n",
    "                neut_spell.append(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "            except IndexError:\n",
    "                neut_spell.append(word)\n",
    "        else:\n",
    "            neut_spell.append(word)\n",
    "    neut_spell = [word for word in neut_spell if not word in stopwords.words('english')]\n",
    "\n",
    "    pos_tokens = word_tokenize(pos_text)\n",
    "    pos_spell = list()\n",
    "    for word in pos_tokens:\n",
    "        if len(word) > 1:\n",
    "            temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                                    set(ngrams(w, 2))),w)\n",
    "                    for w in correct_words if w[0]==word[0]]\n",
    "            try:\n",
    "                pos_spell.append(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "            except IndexError:\n",
    "                pos_spell.append(word)\n",
    "        else:\n",
    "            pos_spell.append(word)\n",
    "    pos_spell = [word for word in pos_spell if not word in stopwords.words('english')]\n",
    "\n",
    "    neg_words = Counter(neg_spell)\n",
    "    neut_words = Counter(neut_spell)\n",
    "    pos_words = Counter(pos_spell)\n",
    "    \n",
    "    unic_words = list(set(neg_words.keys()) | set(neut_words.keys()) | set(pos_words.keys()))\n",
    "\n",
    "    neg_exist_index = 0\n",
    "    neut_exist_index = 1\n",
    "    pos_exist_index = 2\n",
    "    neg_count_index = 3\n",
    "    neut_count_index = 4\n",
    "    pos_count_index = 5\n",
    "    word_count_index = 6\n",
    "    neg_tfidf_index = 7\n",
    "    neut_tfidf_index = 8\n",
    "    pos_tfidf_index = 9\n",
    "\n",
    "    df = np.zeros((len(unic_words), 10))\n",
    "    for i, word in enumerate(unic_words):\n",
    "        if word in neg_words.keys():\n",
    "            df[i,neg_exist_index] = 1\n",
    "            df[i,neg_count_index] = neg_words[word]\n",
    "        if word in neut_words.keys():\n",
    "            df[i,neut_exist_index] = 1\n",
    "            df[i,neut_count_index] = neut_words[word]\n",
    "        if word in pos_words.keys():\n",
    "            df[i,pos_exist_index] = 1\n",
    "            df[i,pos_count_index] = pos_words[word]\n",
    "\n",
    "    df[:,word_count_index] = df[:,neg_count_index] + df[:,neut_count_index] + df[:,pos_count_index]\n",
    "    df[:,neg_tfidf_index] = df[:,neg_count_index] / df[:,word_count_index]\n",
    "    df[:,neut_tfidf_index] = df[:,neut_count_index] / df[:,word_count_index]\n",
    "    df[:,pos_tfidf_index] = df[:,pos_count_index] / df[:,word_count_index]\n",
    "\n",
    "    spell_df = pd.DataFrame(df, columns=[\n",
    "        'Negative', 'Neutral', 'Positive',\n",
    "        'Negative counts', 'Neutral counts', 'Positive counts', 'Word counts',\n",
    "        'Negative TFIDF', 'Neutral TFIDF', 'Positive TFIDF'])\n",
    "    spell_df[\"word\"] = unic_words\n",
    "    return spell_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Узнаем, как называются остальные файлы, содержащие исходный набор данных<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mp00_tweets.zip\u001b[0m*         \u001b[01;32mprocessedNeutral.csv\u001b[0m*\n",
      "\u001b[01;32mprocessedNegative.csv\u001b[0m*  \u001b[01;32mprocessedPositive.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Создадим набор данных для обучения<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative counts</th>\n",
       "      <th>Neutral counts</th>\n",
       "      <th>Positive counts</th>\n",
       "      <th>Word counts</th>\n",
       "      <th>Negative TFIDF</th>\n",
       "      <th>Neutral TFIDF</th>\n",
       "      <th>Positive TFIDF</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Comitium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>storied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Terrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>restrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>arse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5865</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bontok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>longe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5867</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120-odd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5868 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Negative  Neutral  Positive  Negative counts  Neutral counts  \\\n",
       "0          1.0      0.0       0.0              4.0             0.0   \n",
       "1          0.0      1.0       0.0              0.0             1.0   \n",
       "2          1.0      0.0       0.0              1.0             0.0   \n",
       "3          1.0      0.0       0.0              3.0             0.0   \n",
       "4          0.0      0.0       1.0              0.0             0.0   \n",
       "...        ...      ...       ...              ...             ...   \n",
       "5863       0.0      1.0       0.0              0.0             1.0   \n",
       "5864       1.0      0.0       0.0              2.0             0.0   \n",
       "5865       0.0      1.0       0.0              0.0             1.0   \n",
       "5866       0.0      1.0       0.0              0.0             1.0   \n",
       "5867       0.0      1.0       0.0              0.0             1.0   \n",
       "\n",
       "      Positive counts  Word counts  Negative TFIDF  Neutral TFIDF  \\\n",
       "0                 0.0          4.0             1.0            0.0   \n",
       "1                 0.0          1.0             0.0            1.0   \n",
       "2                 0.0          1.0             1.0            0.0   \n",
       "3                 0.0          3.0             1.0            0.0   \n",
       "4                 1.0          1.0             0.0            0.0   \n",
       "...               ...          ...             ...            ...   \n",
       "5863              0.0          1.0             0.0            1.0   \n",
       "5864              0.0          2.0             1.0            0.0   \n",
       "5865              0.0          1.0             0.0            1.0   \n",
       "5866              0.0          1.0             0.0            1.0   \n",
       "5867              0.0          1.0             0.0            1.0   \n",
       "\n",
       "      Positive TFIDF      word  \n",
       "0                0.0        un  \n",
       "1                0.0  Comitium  \n",
       "2                0.0   storied  \n",
       "3                0.0  Terrance  \n",
       "4                1.0      safe  \n",
       "...              ...       ...  \n",
       "5863             0.0  restrain  \n",
       "5864             0.0      arse  \n",
       "5865             0.0    Bontok  \n",
       "5866             0.0     longe  \n",
       "5867             0.0   120-odd  \n",
       "\n",
       "[5868 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = ('data/processedNegative.csv', 'data/processedNeutral.csv', 'data/processedPositive.csv')\n",
    "misspel_df = spell_file_to_df(file_names)\n",
    "misspel_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Сохраним полученный набор данных, чтобы уменьшить ожидание при демонтсрации<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspel_df.to_csv('data/misspel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspel_df = pd.read_csv('data/misspel.csv')\n",
    "unic_words = misspel_df.word.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['un',\n",
       " 'Comitium',\n",
       " 'storied',\n",
       " 'Terrance',\n",
       " 'safe',\n",
       " 'needer',\n",
       " 'Ike',\n",
       " 'Chane',\n",
       " 'nil',\n",
       " 'param',\n",
       " 'admit',\n",
       " 'beginning',\n",
       " 'haw',\n",
       " 'mastermind',\n",
       " 'decided',\n",
       " 'whole',\n",
       " 'accidental',\n",
       " 'P',\n",
       " 'free',\n",
       " 'legitimate',\n",
       " 'naked',\n",
       " 'Sal',\n",
       " 'Bat',\n",
       " 'formerly',\n",
       " 'watching',\n",
       " '900',\n",
       " 'stipend',\n",
       " 'horizon',\n",
       " 'presumably',\n",
       " 'Gamelion',\n",
       " 'lorry',\n",
       " 'information',\n",
       " 'wipe',\n",
       " 'anniversary',\n",
       " 'seedage',\n",
       " 'smoked',\n",
       " 'voice',\n",
       " 'prove',\n",
       " 'Rivina',\n",
       " 'tane',\n",
       " 'Pianola',\n",
       " 'gun',\n",
       " 'immigrant',\n",
       " '.41',\n",
       " 'spat',\n",
       " 'regulator',\n",
       " 'estrange',\n",
       " 'wondering',\n",
       " 'tamperer',\n",
       " 'wont',\n",
       " 'chilling',\n",
       " '3.41',\n",
       " 'stunner',\n",
       " 'stun',\n",
       " 'aimer',\n",
       " 'mobbish',\n",
       " 'quite',\n",
       " 'display',\n",
       " 'Albright',\n",
       " 'clarify',\n",
       " 'hungry',\n",
       " 'Umatilla',\n",
       " 'wifie',\n",
       " 'wave',\n",
       " 'Diego',\n",
       " 'incident',\n",
       " 'Yun',\n",
       " 'Somali',\n",
       " 'discretionary',\n",
       " 'Rajendra',\n",
       " 'quickener',\n",
       " 'Mariana',\n",
       " 'embarrass',\n",
       " '9',\n",
       " 'Trichina',\n",
       " 'vale',\n",
       " 'revive',\n",
       " 'Sho',\n",
       " \"'A7a\",\n",
       " 'Xyleborus',\n",
       " 'medal',\n",
       " 'Equus',\n",
       " 'waffle',\n",
       " 'forward',\n",
       " 'Sydneyite',\n",
       " 'spoil',\n",
       " 'sima',\n",
       " 'J',\n",
       " 'snow',\n",
       " 'mull',\n",
       " 'happen',\n",
       " 'Catostomus',\n",
       " 'Diana',\n",
       " 'Justicia',\n",
       " 'Astragalus',\n",
       " 'crisis',\n",
       " 'giraffe',\n",
       " 'reaching',\n",
       " 'Studite',\n",
       " 'age',\n",
       " 'weaken',\n",
       " 'barse',\n",
       " 'Bill',\n",
       " 'someone',\n",
       " 'iconoclasm',\n",
       " 'Boothian',\n",
       " 'fundi',\n",
       " 'Oldenburg',\n",
       " 'insoul',\n",
       " 'Stilbella',\n",
       " 'rusty',\n",
       " 'Holly',\n",
       " 'media',\n",
       " 'arrest',\n",
       " 'muga',\n",
       " 'Paris',\n",
       " 'clash',\n",
       " 'spaying',\n",
       " 'flu',\n",
       " 'frustration',\n",
       " ':30',\n",
       " 'Ophiuchid',\n",
       " 'verdict',\n",
       " 'refusing',\n",
       " 'prayer',\n",
       " 'ruling',\n",
       " 'Confucian',\n",
       " 'Ordovian',\n",
       " 'number',\n",
       " 'different',\n",
       " 'blame',\n",
       " 'coming',\n",
       " 'edition',\n",
       " 'return',\n",
       " 'raki',\n",
       " '|',\n",
       " 'Terminalia',\n",
       " 'Dail',\n",
       " 'tonight',\n",
       " 'Oneida',\n",
       " 'bodied',\n",
       " '5/6',\n",
       " 'term',\n",
       " 'state',\n",
       " 'coach',\n",
       " 'writing',\n",
       " 'Reggie',\n",
       " 'awful',\n",
       " 'kop',\n",
       " 'twitterboned',\n",
       " 'fra',\n",
       " 'rank',\n",
       " 'youngish',\n",
       " 'Neil',\n",
       " 'Disporum',\n",
       " 'camera',\n",
       " 'air',\n",
       " 'Halleyan',\n",
       " 'unfollowed',\n",
       " 'Advaita',\n",
       " 'lane',\n",
       " 'cause',\n",
       " 'accuracy',\n",
       " 'involved',\n",
       " 'Tennysonian',\n",
       " 'Leonese',\n",
       " '.32',\n",
       " '2k',\n",
       " 'Semang',\n",
       " 'ad',\n",
       " \"'ve\",\n",
       " 'birthplace',\n",
       " 'Unionidae',\n",
       " '.17',\n",
       " 'stunning',\n",
       " 'pusher',\n",
       " 'threat',\n",
       " 'Kushshu',\n",
       " 'Beaumontia',\n",
       " 'speaker',\n",
       " '100m',\n",
       " 'blue',\n",
       " 'Olinia',\n",
       " 'military',\n",
       " 'Iroha',\n",
       " 'call',\n",
       " 'Metaurus',\n",
       " 'nephew',\n",
       " 'Rollinia',\n",
       " 'Stoic',\n",
       " 'Roger',\n",
       " 'patience',\n",
       " 'village',\n",
       " 'accomplish',\n",
       " 'broken',\n",
       " 'litchi',\n",
       " 'fossil',\n",
       " 'war',\n",
       " 'confident',\n",
       " 'Bucky',\n",
       " 'Hibito',\n",
       " 'vanity',\n",
       " 'guilty',\n",
       " 'funny',\n",
       " 'surge',\n",
       " 'mobile',\n",
       " 'menace',\n",
       " 'g',\n",
       " 'bishopship',\n",
       " 'Normanesque',\n",
       " 'stripe',\n",
       " 'story',\n",
       " 'Moslemin',\n",
       " 'millennial',\n",
       " 'basketball',\n",
       " 'visit',\n",
       " 'within',\n",
       " 'gate',\n",
       " 'homage',\n",
       " 'custody',\n",
       " 'organic',\n",
       " 'undone',\n",
       " 'signal',\n",
       " 'Senci',\n",
       " 'polythene',\n",
       " 'revisit',\n",
       " 'condole',\n",
       " 'ego',\n",
       " 'Constance',\n",
       " 'prefer',\n",
       " '800',\n",
       " 'test',\n",
       " 'cheap',\n",
       " 'Bluebeard',\n",
       " 'gan',\n",
       " 'I',\n",
       " 'Knesset',\n",
       " 'pitch',\n",
       " 'Liukiu',\n",
       " 'majority',\n",
       " 'smile',\n",
       " 'inroad',\n",
       " 'satisfied',\n",
       " 'Cours',\n",
       " 'vocal',\n",
       " 'caged',\n",
       " 'edit',\n",
       " 'purchase',\n",
       " 'legend',\n",
       " 'U',\n",
       " 'ply',\n",
       " 'middleman',\n",
       " 'goodly',\n",
       " 'grandfilial',\n",
       " 'Fingall',\n",
       " 'percent',\n",
       " 'lucanid',\n",
       " 'cosmic',\n",
       " 'preventingly',\n",
       " 'adult',\n",
       " 'learnt',\n",
       " 'meadow',\n",
       " 'session',\n",
       " 'president',\n",
       " 'defeat',\n",
       " 'Bullom',\n",
       " 'Asiatically',\n",
       " 'Vertebrata',\n",
       " 'usings',\n",
       " 'Ita',\n",
       " 'Coumarouna',\n",
       " 'driver',\n",
       " 'helper',\n",
       " 'Ana',\n",
       " 'Ric',\n",
       " 'Hades',\n",
       " 'sweater',\n",
       " 'Regga',\n",
       " 'Hapi',\n",
       " 'mandatory',\n",
       " 'sexually',\n",
       " 'bulk',\n",
       " 'arms',\n",
       " 'phase',\n",
       " 'new',\n",
       " 'proof',\n",
       " 'quizzy',\n",
       " 'Dentaria',\n",
       " 'shadow',\n",
       " 'facility',\n",
       " 'bot',\n",
       " 'riding',\n",
       " 'grim',\n",
       " '70k',\n",
       " 'Uchee',\n",
       " 'exceptional',\n",
       " 'give',\n",
       " 'Protracheata',\n",
       " 'browser',\n",
       " 'Whit',\n",
       " 'Houstonia',\n",
       " 'foreign',\n",
       " 'Lester',\n",
       " 'design',\n",
       " 'Sodom',\n",
       " 'lock',\n",
       " '10',\n",
       " 'appointe',\n",
       " '26/117',\n",
       " 'entry',\n",
       " 'play',\n",
       " 'Tod',\n",
       " 'Heather',\n",
       " 'victim',\n",
       " 'hilding',\n",
       " 'padding',\n",
       " 'Pollux',\n",
       " 'tenderer',\n",
       " 'astroid',\n",
       " 'Arundo',\n",
       " 'Chequers',\n",
       " 'finish',\n",
       " 'forth',\n",
       " 'also',\n",
       " 'education',\n",
       " 'chocolate',\n",
       " 'field',\n",
       " 'Yeshiva',\n",
       " 'mince',\n",
       " 'ten',\n",
       " 'turn',\n",
       " 'smooth',\n",
       " 'sny',\n",
       " 'indan',\n",
       " 'sealed',\n",
       " 'indicia',\n",
       " 'debby',\n",
       " 'sport',\n",
       " 'especially',\n",
       " 'engaged',\n",
       " 'terror',\n",
       " 'Pele',\n",
       " 'sociocrat',\n",
       " 'anticipate',\n",
       " 'senior',\n",
       " 'estimate',\n",
       " 'sports',\n",
       " 'persist',\n",
       " 'enjoy',\n",
       " 'consolidation',\n",
       " 'Reub',\n",
       " 'six',\n",
       " 'braces',\n",
       " 'uproar',\n",
       " '2016-17',\n",
       " 'census',\n",
       " 'troop',\n",
       " 'Thursday',\n",
       " 'worst',\n",
       " 'Parridae',\n",
       " 'Wagnerite',\n",
       " 'Mecklenburgian',\n",
       " 'accused',\n",
       " 'billboard',\n",
       " 'nicely',\n",
       " 'goat',\n",
       " 'Chien',\n",
       " 'Mysticete',\n",
       " 'forecast',\n",
       " 'Eoghanacht',\n",
       " 'Sinto',\n",
       " 'noncontact',\n",
       " 'Economite',\n",
       " 'aa',\n",
       " 'applyingly',\n",
       " 'sanitary',\n",
       " 'appropriate',\n",
       " 'abut',\n",
       " 'merge',\n",
       " 'vehicle',\n",
       " 'wake',\n",
       " 'Ho',\n",
       " 'neighbourship',\n",
       " 'surround',\n",
       " 'industrialist',\n",
       " 'alarm',\n",
       " 'expansion',\n",
       " 'G',\n",
       " 'unlink',\n",
       " 'communalism',\n",
       " 'Ban',\n",
       " 'Teague',\n",
       " 'elusive',\n",
       " 'graben',\n",
       " 'point',\n",
       " 'decline',\n",
       " 'brotherhood',\n",
       " '101',\n",
       " 'yesso',\n",
       " 'latah',\n",
       " 'Hitlerite',\n",
       " 'fight',\n",
       " 'easter',\n",
       " 'eminent',\n",
       " 'old',\n",
       " '.48',\n",
       " 'thanks',\n",
       " 'chimango',\n",
       " '........',\n",
       " 'encounter',\n",
       " 'attachment',\n",
       " 'needles',\n",
       " 'Venkata',\n",
       " 'Toda',\n",
       " 'tale',\n",
       " 'Salopian',\n",
       " 'leftist',\n",
       " '38yr',\n",
       " 'Bengali',\n",
       " 'undercurrent',\n",
       " 'across',\n",
       " 'withdrawn',\n",
       " 'Lif',\n",
       " 'video',\n",
       " 'mass',\n",
       " '22.11bn',\n",
       " 'red',\n",
       " 'engrossed',\n",
       " 'gang',\n",
       " 'bib',\n",
       " 'edge',\n",
       " 'edgy',\n",
       " 'beach',\n",
       " 'hala',\n",
       " '115',\n",
       " 'Jem',\n",
       " 'color',\n",
       " 'understanding',\n",
       " 'Vladimir',\n",
       " 'Nayar',\n",
       " 'action',\n",
       " 'murderer',\n",
       " 'tracker',\n",
       " 'telecode',\n",
       " 'stay',\n",
       " 'tired',\n",
       " 'hijack',\n",
       " 'Clio',\n",
       " 'horrific',\n",
       " 'ear',\n",
       " 'quashey',\n",
       " 'integrity',\n",
       " 'Will',\n",
       " 'consequential',\n",
       " 'roost',\n",
       " 'Scientist',\n",
       " 'win',\n",
       " 'Karl',\n",
       " 'Louis',\n",
       " '.36',\n",
       " 'ambition',\n",
       " 'pace',\n",
       " 'poison',\n",
       " 'Navaho',\n",
       " 'Mimus',\n",
       " 'Haraya',\n",
       " 'layout',\n",
       " 'extension',\n",
       " 'pedestrian',\n",
       " '1/3',\n",
       " 'tend',\n",
       " 'review',\n",
       " 'bigha',\n",
       " 'stone',\n",
       " 'panic',\n",
       " 'retter',\n",
       " 'Heliaea',\n",
       " 'Everyman',\n",
       " 'muddle',\n",
       " 'author',\n",
       " 'pussy',\n",
       " 'pitfall',\n",
       " 'turtle',\n",
       " 'seriously',\n",
       " 'man',\n",
       " 'could',\n",
       " 'recommendee',\n",
       " 'Maurice',\n",
       " 'Nomeidae',\n",
       " 'Arean',\n",
       " '610',\n",
       " 'steam',\n",
       " 'donate',\n",
       " 'wedding',\n",
       " 'gentian',\n",
       " 'pale',\n",
       " 'Farsi',\n",
       " 'menu',\n",
       " 'largess',\n",
       " '.51',\n",
       " 'south',\n",
       " 'tong',\n",
       " 'dong',\n",
       " 'aggregator',\n",
       " 'prison',\n",
       " 'storm',\n",
       " 'Britain',\n",
       " 'language',\n",
       " 'trub',\n",
       " 'nervous',\n",
       " 'conclude',\n",
       " 'fabric',\n",
       " 'Whiglet',\n",
       " 'integral',\n",
       " 'true',\n",
       " 'cost',\n",
       " 'invite',\n",
       " 'Krishna',\n",
       " 'pointless',\n",
       " 'hubby',\n",
       " 'sufficient',\n",
       " 'seeing',\n",
       " 'bin',\n",
       " 'classroom',\n",
       " 'Woody',\n",
       " '5.7',\n",
       " 'worlded',\n",
       " 'scored',\n",
       " 'Haiathalah',\n",
       " '57.36',\n",
       " '24.',\n",
       " 'neutral',\n",
       " 'latten',\n",
       " 'resign',\n",
       " 'declared',\n",
       " 'Unrra',\n",
       " 'Kurd',\n",
       " 'beating',\n",
       " 'Achates',\n",
       " 'hard',\n",
       " 'tradition',\n",
       " 'Lea',\n",
       " 'illegal',\n",
       " '431',\n",
       " 'Sol',\n",
       " 'Lateran',\n",
       " 'act',\n",
       " 'Adamic',\n",
       " 'Alle',\n",
       " 'registered',\n",
       " 'land',\n",
       " 'fish',\n",
       " '7',\n",
       " 'Choes',\n",
       " 'toy',\n",
       " 'sauce',\n",
       " 'hooked',\n",
       " 'none',\n",
       " 'M',\n",
       " 'Aani',\n",
       " 'laying',\n",
       " 'dungeon',\n",
       " \"'nationalist\",\n",
       " 'theatry',\n",
       " 'Ona',\n",
       " 'pensy',\n",
       " 'redevise',\n",
       " 'Sri',\n",
       " 'accommodation',\n",
       " 'offering',\n",
       " 'fog',\n",
       " 'unhappiness',\n",
       " '84',\n",
       " 'Howea',\n",
       " 'actually',\n",
       " 'Hispa',\n",
       " 'norm',\n",
       " 'Old',\n",
       " 'tonsil',\n",
       " '3pm',\n",
       " 'journalist',\n",
       " 'Berteroa',\n",
       " 'amity',\n",
       " 'announce',\n",
       " 'send',\n",
       " 'hawk',\n",
       " 'express',\n",
       " '150',\n",
       " 'Marattia',\n",
       " 'analytics',\n",
       " 'liesh',\n",
       " 'Theridion',\n",
       " 'teacher',\n",
       " \"'formerly\",\n",
       " 'Mer',\n",
       " 'frown',\n",
       " 'tannyl',\n",
       " 'odoom',\n",
       " 'Charlie',\n",
       " 'block',\n",
       " 'figment',\n",
       " 'Factice',\n",
       " 'door',\n",
       " 'dame',\n",
       " 'Lee',\n",
       " 'disappear',\n",
       " 'calendar',\n",
       " 'Dadayag',\n",
       " 'affair',\n",
       " 'Monday',\n",
       " 'controllingly',\n",
       " 'saying',\n",
       " 'unruly',\n",
       " 'tied',\n",
       " 'poll',\n",
       " 'Anoplotherium',\n",
       " 'night',\n",
       " 'sang',\n",
       " 'applicable',\n",
       " 'Job',\n",
       " 'active',\n",
       " 'district',\n",
       " 'god',\n",
       " 'pervert',\n",
       " 'ended',\n",
       " 'rule',\n",
       " 'busy',\n",
       " 'courage',\n",
       " 'backup',\n",
       " 'miss',\n",
       " '15',\n",
       " 'wishing',\n",
       " 'college',\n",
       " 'wronged',\n",
       " 'yore',\n",
       " 'torture',\n",
       " 'Michel',\n",
       " 'double',\n",
       " 'crop',\n",
       " 'hospital',\n",
       " 'yang',\n",
       " 'eel',\n",
       " 'mean',\n",
       " 'Boyd',\n",
       " 'terrorist',\n",
       " 'Satan',\n",
       " 'recolor',\n",
       " 'clue',\n",
       " 'Net',\n",
       " 'French',\n",
       " 'inaugural',\n",
       " 'Goodenia',\n",
       " 'electee',\n",
       " 'facia',\n",
       " 'pollution',\n",
       " 'Mr',\n",
       " '.18',\n",
       " 'musical',\n",
       " 'Waterberg',\n",
       " '13RW',\n",
       " 'shipper',\n",
       " 'ticket',\n",
       " 'Bangala',\n",
       " 'dynamic',\n",
       " 'irisroot',\n",
       " 'Stanley',\n",
       " 'Sumerian',\n",
       " '3400',\n",
       " 'tuned',\n",
       " 'puppet',\n",
       " 'varus',\n",
       " 'looker',\n",
       " 'schedule',\n",
       " 'Scot',\n",
       " 'favorite',\n",
       " 'German',\n",
       " 'blackmailer',\n",
       " 'Gammarus',\n",
       " 'bahur',\n",
       " '1.0',\n",
       " 'Fianna',\n",
       " 'basis',\n",
       " 'hogwort',\n",
       " 'feed',\n",
       " 'Argentina',\n",
       " 'house',\n",
       " 'overrate',\n",
       " 'social',\n",
       " 'demonetize',\n",
       " 'Southerner',\n",
       " 'arm',\n",
       " 'legitimacy',\n",
       " 'known',\n",
       " 'worry',\n",
       " 'Wolf',\n",
       " 'compressed',\n",
       " 'soured',\n",
       " 'stealing',\n",
       " 'Mo',\n",
       " 'Nathan',\n",
       " 'rouse',\n",
       " 'constitute',\n",
       " 'needs',\n",
       " 'imagine',\n",
       " 'behind',\n",
       " 'bazaar',\n",
       " 'sook',\n",
       " 'trucks',\n",
       " 'hello',\n",
       " 'humble',\n",
       " 'youngling',\n",
       " 'key',\n",
       " 'tunnel',\n",
       " 'belonging',\n",
       " 'upgrade',\n",
       " 'Dob',\n",
       " 'assigned',\n",
       " 'Didachist',\n",
       " 'diplomat',\n",
       " 'havent',\n",
       " 'froze',\n",
       " 'reform',\n",
       " 'recipe',\n",
       " 'Gum',\n",
       " 'dirty',\n",
       " 'reflection',\n",
       " 'protective',\n",
       " 'charm',\n",
       " 'fail',\n",
       " 'publisher',\n",
       " 'campus',\n",
       " 'Gentoo',\n",
       " 'hash',\n",
       " 'pill',\n",
       " 'statement',\n",
       " 'times',\n",
       " 'Minnie',\n",
       " 'economics',\n",
       " 'chief',\n",
       " '0xx-9',\n",
       " 'Aperu',\n",
       " 'Ingram',\n",
       " 'area',\n",
       " '100-yr-old',\n",
       " 'abortin',\n",
       " 'subsidist',\n",
       " 'initiative',\n",
       " 'headband',\n",
       " 'velvet',\n",
       " 'unqualified',\n",
       " 'happiness',\n",
       " 'heart',\n",
       " 'application',\n",
       " 'junker',\n",
       " 'taken',\n",
       " 'raking',\n",
       " 'neighbor',\n",
       " 'mighty',\n",
       " 'America',\n",
       " 'Chin',\n",
       " 'place',\n",
       " 'Greg',\n",
       " 'Congreso',\n",
       " 'attend',\n",
       " 'television',\n",
       " 'subsurface',\n",
       " 'lend',\n",
       " 'fun',\n",
       " 'included',\n",
       " 'belated',\n",
       " 'slept',\n",
       " 'ah',\n",
       " 'charge',\n",
       " 'plus',\n",
       " 'killer',\n",
       " 'fit',\n",
       " 'Sus',\n",
       " 'reinvent',\n",
       " 'Pigmy',\n",
       " 'ammunition',\n",
       " 'Rome',\n",
       " 'pinned',\n",
       " 'suspended',\n",
       " 'Washington',\n",
       " 'Lamba',\n",
       " 'avenger',\n",
       " 'Primus',\n",
       " 'assembly',\n",
       " 'Ravenelia',\n",
       " 'Lavinia',\n",
       " 'connected',\n",
       " 'philippize',\n",
       " 'Baggara',\n",
       " '30+',\n",
       " 'Adigranth',\n",
       " 'stood',\n",
       " 'pap',\n",
       " 'measured',\n",
       " 'recruit',\n",
       " '3hrs',\n",
       " '4-20',\n",
       " 'creepy',\n",
       " 'Visaya',\n",
       " 'shoeshop',\n",
       " 'Baluba',\n",
       " 'Sinae',\n",
       " 'bant',\n",
       " 'album',\n",
       " 'chain',\n",
       " 'Zeus',\n",
       " 'stable',\n",
       " 'screen',\n",
       " 'kiss',\n",
       " 'unspoken',\n",
       " 'sooner',\n",
       " 'brought',\n",
       " 'Megarian',\n",
       " 'becap',\n",
       " 'twitch',\n",
       " 'Psidium',\n",
       " 'small',\n",
       " 'keep',\n",
       " 'oppose',\n",
       " 'explode',\n",
       " 'one',\n",
       " \"'M\",\n",
       " '12-15',\n",
       " 'Baal',\n",
       " 'morning',\n",
       " 'Helen',\n",
       " 'amid',\n",
       " 'Jam',\n",
       " 'Rembrandt',\n",
       " 'Eve',\n",
       " 'rung',\n",
       " \"'adopted\",\n",
       " 'theory',\n",
       " 'cox',\n",
       " 'map',\n",
       " 'alish',\n",
       " 'Ferrara',\n",
       " 'hearse',\n",
       " 'found',\n",
       " 'pass',\n",
       " 'current',\n",
       " 'explain',\n",
       " 'pushing',\n",
       " 'punishment',\n",
       " '.47',\n",
       " 'allegation',\n",
       " 'artificial',\n",
       " 'prime',\n",
       " 'Sudan',\n",
       " 'reactor',\n",
       " 'crackdown',\n",
       " 'approve',\n",
       " 'intake',\n",
       " 'Dechlog',\n",
       " 'slap',\n",
       " 'anybody',\n",
       " '7pm.1',\n",
       " 'inbond',\n",
       " 'siltation',\n",
       " 'pseudo',\n",
       " '45',\n",
       " 'commission',\n",
       " 'Supai',\n",
       " '05/70',\n",
       " 'olive',\n",
       " 'Cabinda',\n",
       " '247',\n",
       " 'sub',\n",
       " 'coastal',\n",
       " 'Maku',\n",
       " 'Telegn',\n",
       " 'Sarcoptes',\n",
       " 'certain',\n",
       " 'sight',\n",
       " 'tour',\n",
       " '3.4',\n",
       " 'contribution',\n",
       " 'Jessica',\n",
       " 'store',\n",
       " 'unstable',\n",
       " 'hostile',\n",
       " 'Kyle',\n",
       " 'given',\n",
       " 'paper',\n",
       " 'mayor',\n",
       " 'confession',\n",
       " '30',\n",
       " 'controller',\n",
       " 'wonder',\n",
       " 'fracas',\n",
       " 'Liverpudlian',\n",
       " 'Goth',\n",
       " '11am',\n",
       " 'Hevea',\n",
       " 'batsman',\n",
       " 'Pelops',\n",
       " 'livid',\n",
       " '.35',\n",
       " '200000',\n",
       " 'available',\n",
       " '-D',\n",
       " 'nonsecret',\n",
       " 'noon',\n",
       " 'lighting',\n",
       " 'Ok',\n",
       " 'hyper',\n",
       " 'roommate',\n",
       " 'joust',\n",
       " 'Templar',\n",
       " 'request',\n",
       " 'ming',\n",
       " 'vent',\n",
       " 'meant',\n",
       " \"'architecture\",\n",
       " 'standing',\n",
       " 'price',\n",
       " 'relief',\n",
       " 'progress',\n",
       " 'Nama',\n",
       " 'graveyard',\n",
       " 'reading',\n",
       " 'Madeira',\n",
       " 'lobby',\n",
       " 'Champa',\n",
       " 'parallel',\n",
       " 'solo',\n",
       " 'Tamarindus',\n",
       " 'scholar',\n",
       " '63',\n",
       " 'min',\n",
       " 'planting',\n",
       " 'Bepaint',\n",
       " 'Malayalam',\n",
       " 'idea',\n",
       " 'Dard',\n",
       " 'cover',\n",
       " 'attacker',\n",
       " 'Lead',\n",
       " 'contact',\n",
       " 'Orca',\n",
       " 'nonhero',\n",
       " '50k',\n",
       " 'subsidiarie',\n",
       " 'Basella',\n",
       " 'Amanda',\n",
       " 'soft',\n",
       " 'raffle',\n",
       " 'response',\n",
       " 'helder',\n",
       " 'healer',\n",
       " 'sterelminthic',\n",
       " 'member',\n",
       " 'hooping',\n",
       " '7Days',\n",
       " 'Deutzia',\n",
       " 'Fumariaceae',\n",
       " '=Thumbnail',\n",
       " 'governor',\n",
       " 'Mauri',\n",
       " 'cent',\n",
       " 'Pisan',\n",
       " 'lower',\n",
       " 'guideline',\n",
       " '401',\n",
       " '.38',\n",
       " 'Rab',\n",
       " 'Bongo',\n",
       " 'Spencer',\n",
       " 'dynastid',\n",
       " 'prey',\n",
       " 'academial',\n",
       " 'pen',\n",
       " 'defiance',\n",
       " '.16',\n",
       " 'Chandi',\n",
       " 'arrangement',\n",
       " 'wished',\n",
       " 'Oryzopsis',\n",
       " 'fabulous',\n",
       " 'weekend',\n",
       " 'conversation',\n",
       " 'hatting',\n",
       " 'Litsea',\n",
       " 'reward',\n",
       " 'irade',\n",
       " 'coachee',\n",
       " 'build',\n",
       " 'inflow',\n",
       " 'plead',\n",
       " 'March',\n",
       " 'creeping',\n",
       " 'Friday',\n",
       " 'kohl',\n",
       " 'scrapping',\n",
       " 'Dale',\n",
       " 'streak',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unic_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Узнаем полученную точность модели<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/val/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "word_exist_accuracy_score = model_selection_word_exist(misspel_df, unic_words)\n",
    "word_count_accuracy_score = model_selection_word_count(misspel_df, unic_words)\n",
    "tfidf_accuracy_score = model_selection_tfidf(misspel_df, unic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score by word exist: 0.5170357751277683\n",
      "Accuracy score by word count: 0.9522998296422487\n",
      "Fccuracy score by tfidf: 0.5017035775127768\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Accuracy score by word exist: {word_exist_accuracy_score}\n",
    "Accuracy score by word count: {word_count_accuracy_score}\n",
    "Fccuracy score by tfidf: {tfidf_accuracy_score}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
